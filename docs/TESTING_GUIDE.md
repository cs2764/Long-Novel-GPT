# æµ‹è¯•å’ŒéªŒè¯æŒ‡å—

æœ¬æ–‡æ¡£æä¾›äº†Long-Novel-GPTé¡¹ç›®çš„å®Œæ•´æµ‹è¯•å’ŒéªŒè¯æŒ‡å—ï¼ŒåŒ…æ‹¬å•å…ƒæµ‹è¯•ã€é›†æˆæµ‹è¯•ã€æ€§èƒ½æµ‹è¯•ã€APIéªŒè¯ç­‰æ–¹é¢ã€‚

## ğŸ“‹ ç›®å½•

1. [æµ‹è¯•æ¶æ„æ¦‚è¿°](#æµ‹è¯•æ¶æ„æ¦‚è¿°)
2. [æµ‹è¯•ç¯å¢ƒé…ç½®](#æµ‹è¯•ç¯å¢ƒé…ç½®)
3. [å•å…ƒæµ‹è¯•](#å•å…ƒæµ‹è¯•)
4. [é›†æˆæµ‹è¯•](#é›†æˆæµ‹è¯•)
5. [APIæµ‹è¯•](#APIæµ‹è¯•)
6. [æ€§èƒ½æµ‹è¯•](#æ€§èƒ½æµ‹è¯•)
7. [å‰ç«¯æµ‹è¯•](#å‰ç«¯æµ‹è¯•)
8. [è‡ªåŠ¨åŒ–æµ‹è¯•](#è‡ªåŠ¨åŒ–æµ‹è¯•)
9. [æµ‹è¯•æ•°æ®ç®¡ç†](#æµ‹è¯•æ•°æ®ç®¡ç†)
10. [æµ‹è¯•æŠ¥å‘Š](#æµ‹è¯•æŠ¥å‘Š)

## æµ‹è¯•æ¶æ„æ¦‚è¿°

### æµ‹è¯•ç­–ç•¥

```
æµ‹è¯•é‡‘å­—å¡”
    â†‘
    |  ç«¯åˆ°ç«¯æµ‹è¯• (E2E)
    |      â†‘
    |      |  é›†æˆæµ‹è¯•
    |      |      â†‘
    |      |      |  å•å…ƒæµ‹è¯•
    |      |      |      â†‘
    |      |      |      |  é™æ€ä»£ç åˆ†æ
    â†“      â†“      â†“      â†“
  å°‘é‡   ä¸­ç­‰   å¤§é‡   åŸºç¡€
```

### æµ‹è¯•è¦†ç›–èŒƒå›´

- **å•å…ƒæµ‹è¯•**: æ ¸å¿ƒä¸šåŠ¡é€»è¾‘ã€å·¥å…·å‡½æ•°ã€APIæ¥å£
- **é›†æˆæµ‹è¯•**: æ¨¡å—é—´äº¤äº’ã€æ•°æ®åº“æ“ä½œã€å¤–éƒ¨APIè°ƒç”¨
- **æ€§èƒ½æµ‹è¯•**: å“åº”æ—¶é—´ã€å¹¶å‘å¤„ç†ã€å†…å­˜ä½¿ç”¨
- **ç«¯åˆ°ç«¯æµ‹è¯•**: å®Œæ•´ç”¨æˆ·æµç¨‹ã€ç•Œé¢äº¤äº’

### æµ‹è¯•å·¥å…·æ ˆ

```python
# æµ‹è¯•æ¡†æ¶
pytest              # ä¸»è¦æµ‹è¯•æ¡†æ¶
unittest            # Pythonæ ‡å‡†æµ‹è¯•åº“
asyncio             # å¼‚æ­¥æµ‹è¯•æ”¯æŒ

# æ¨¡æ‹Ÿå’Œå­˜æ ¹
pytest-mock        # Mockå¯¹è±¡
responses           # HTTPå“åº”æ¨¡æ‹Ÿ
freezegun           # æ—¶é—´æ¨¡æ‹Ÿ

# è¦†ç›–ç‡æµ‹è¯•
pytest-cov         # ä»£ç è¦†ç›–ç‡
coverage            # è¦†ç›–ç‡æŠ¥å‘Š

# æ€§èƒ½æµ‹è¯•
locust              # è´Ÿè½½æµ‹è¯•
memory_profiler     # å†…å­˜åˆ†æ
pytest-benchmark    # æ€§èƒ½åŸºå‡†æµ‹è¯•

# å‰ç«¯æµ‹è¯•
selenium            # æµè§ˆå™¨è‡ªåŠ¨åŒ–
requests            # HTTPå®¢æˆ·ç«¯æµ‹è¯•
```

## æµ‹è¯•ç¯å¢ƒé…ç½®

### 1. æµ‹è¯•ç¯å¢ƒå®‰è£…

```bash
# å®‰è£…æµ‹è¯•ä¾èµ–
pip install pytest pytest-mock pytest-cov pytest-asyncio
pip install responses freezegun locust selenium
pip install memory_profiler pytest-benchmark

# åˆ›å»ºæµ‹è¯•é…ç½®æ–‡ä»¶
cat > pytest.ini << EOF
[tool:pytest]
testpaths = tests
python_files = test_*.py
python_classes = Test*
python_functions = test_*
addopts = 
    -v
    --tb=short
    --cov=.
    --cov-report=html
    --cov-report=term-missing
    --cov-exclude=tests/*
    --cov-exclude=venv/*
markers =
    unit: Unit tests
    integration: Integration tests
    performance: Performance tests
    slow: Slow tests
    api: API tests
EOF
```

### 2. æµ‹è¯•ç›®å½•ç»“æ„

```
tests/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ conftest.py              # æµ‹è¯•é…ç½®å’Œfixtures
â”œâ”€â”€ unit/                    # å•å…ƒæµ‹è¯•
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_writer.py
â”‚   â”œâ”€â”€ test_config.py
â”‚   â””â”€â”€ test_utils.py
â”œâ”€â”€ integration/             # é›†æˆæµ‹è¯•
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_api.py
â”‚   â”œâ”€â”€ test_database.py
â”‚   â””â”€â”€ test_workflow.py
â”œâ”€â”€ performance/             # æ€§èƒ½æµ‹è¯•
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_load.py
â”‚   â””â”€â”€ test_memory.py
â”œâ”€â”€ frontend/                # å‰ç«¯æµ‹è¯•
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_ui.py
â”‚   â””â”€â”€ test_api_integration.py
â”œâ”€â”€ fixtures/                # æµ‹è¯•æ•°æ®
â”‚   â”œâ”€â”€ test_data.json
â”‚   â””â”€â”€ mock_responses.json
â””â”€â”€ utils/                   # æµ‹è¯•å·¥å…·
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ test_helpers.py
    â””â”€â”€ mock_providers.py
```

### 3. æµ‹è¯•é…ç½®æ–‡ä»¶

```python
# tests/conftest.py
import pytest
import os
import json
from unittest.mock import Mock, patch
from core.writer import Writer
from core.dynamic_config_manager import DynamicConfigManager
from llm_api import ModelConfig

@pytest.fixture
def test_config():
    """æµ‹è¯•é…ç½®fixture"""
    return {
        'DEEPSEEK_API_KEY': 'test-key',
        'OPENAI_API_KEY': 'test-key',
        'MONGODB_URI': 'mongodb://localhost:27017/test_db',
        'ENABLE_MONGODB': 'false',
        'DEBUG': 'true'
    }

@pytest.fixture
def mock_model_config():
    """Mockæ¨¡å‹é…ç½®"""
    return ModelConfig(
        model='test-model',
        api_key='test-key',
        max_tokens=1000,
        base_url='http://localhost:1234/v1'
    )

@pytest.fixture
def mock_writer():
    """Mockå†™ä½œå™¨"""
    writer = Mock(spec=Writer)
    writer.generate_text.return_value = "æµ‹è¯•ç”Ÿæˆçš„æ–‡æœ¬"
    return writer

@pytest.fixture
def test_data_loader():
    """æµ‹è¯•æ•°æ®åŠ è½½å™¨"""
    def load_test_data(filename):
        with open(f'tests/fixtures/{filename}', 'r', encoding='utf-8') as f:
            return json.load(f)
    return load_test_data

@pytest.fixture(scope="session")
def test_app():
    """æµ‹è¯•åº”ç”¨fixture"""
    from backend.app import app
    app.config['TESTING'] = True
    return app

@pytest.fixture
def client(test_app):
    """æµ‹è¯•å®¢æˆ·ç«¯"""
    return test_app.test_client()

@pytest.fixture(autouse=True)
def cleanup_after_test():
    """æµ‹è¯•åæ¸…ç†"""
    yield
    # æ¸…ç†æµ‹è¯•æ•°æ®
    import tempfile
    import shutil
    temp_dir = tempfile.gettempdir()
    for file in os.listdir(temp_dir):
        if file.startswith('test_'):
            try:
                os.remove(os.path.join(temp_dir, file))
            except:
                pass
```

## å•å…ƒæµ‹è¯•

### 1. æ ¸å¿ƒåŠŸèƒ½æµ‹è¯•

```python
# tests/unit/test_writer.py
import pytest
from unittest.mock import Mock, patch
from core.writer import Writer
from core.draft_writer import DraftWriter
from llm_api import ModelConfig

class TestWriter:
    def test_writer_initialization(self):
        """æµ‹è¯•å†™ä½œå™¨åˆå§‹åŒ–"""
        writer = Writer()
        assert writer is not None
        assert hasattr(writer, 'generate_text')
    
    def test_model_config_validation(self):
        """æµ‹è¯•æ¨¡å‹é…ç½®éªŒè¯"""
        # æœ‰æ•ˆé…ç½®
        config = ModelConfig(
            model='test-model',
            api_key='test-key',
            max_tokens=1000
        )
        assert config['model'] == 'test-model'
        assert config['api_key'] == 'test-key'
        assert config['max_tokens'] == 1000
        
        # æ— æ•ˆé…ç½®
        with pytest.raises(ValueError):
            ModelConfig(
                model='test-model',
                # ç¼ºå°‘å¿…éœ€çš„api_key
                max_tokens=1000
            )
    
    @patch('llm_api.stream_chat')
    def test_text_generation(self, mock_stream_chat):
        """æµ‹è¯•æ–‡æœ¬ç”Ÿæˆ"""
        # è®¾ç½®mockè¿”å›å€¼
        mock_stream_chat.return_value = iter([
            Mock(content="æµ‹è¯•ç”Ÿæˆçš„æ–‡æœ¬")
        ])
        
        writer = Writer()
        result = writer.generate_text("æµ‹è¯•æç¤ºè¯")
        
        assert result is not None
        assert "æµ‹è¯•ç”Ÿæˆçš„æ–‡æœ¬" in result
        mock_stream_chat.assert_called_once()
    
    def test_chunk_processing(self):
        """æµ‹è¯•æ–‡æœ¬å—å¤„ç†"""
        writer = DraftWriter()
        
        # æµ‹è¯•æ•°æ®
        chunks = [
            ("ç« èŠ‚1", "åŸå§‹å†…å®¹1"),
            ("ç« èŠ‚2", "åŸå§‹å†…å®¹2")
        ]
        
        processed = writer.process_chunks(chunks)
        
        assert len(processed) == 2
        assert all(chunk[0] for chunk in processed)  # æ ‡é¢˜ä¸ä¸ºç©º
        assert all(chunk[1] for chunk in processed)  # å†…å®¹ä¸ä¸ºç©º
    
    def test_error_handling(self):
        """æµ‹è¯•é”™è¯¯å¤„ç†"""
        writer = Writer()
        
        # æµ‹è¯•ç©ºè¾“å…¥
        with pytest.raises(ValueError):
            writer.generate_text("")
        
        # æµ‹è¯•è¶…é•¿è¾“å…¥
        with pytest.raises(ValueError):
            writer.generate_text("x" * 10000)
```

### 2. é…ç½®ç®¡ç†æµ‹è¯•

```python
# tests/unit/test_config.py
import pytest
import json
import tempfile
from unittest.mock import patch, mock_open
from core.dynamic_config_manager import DynamicConfigManager

class TestDynamicConfigManager:
    def test_config_initialization(self):
        """æµ‹è¯•é…ç½®åˆå§‹åŒ–"""
        manager = DynamicConfigManager()
        assert manager is not None
        assert manager.get_current_provider() is not None
    
    def test_provider_config_update(self):
        """æµ‹è¯•æä¾›å•†é…ç½®æ›´æ–°"""
        manager = DynamicConfigManager()
        
        # æµ‹è¯•é…ç½®æ›´æ–°
        config = {
            'api_key': 'new-test-key',
            'base_url': 'https://api.test.com'
        }
        
        result = manager.update_provider_config('test_provider', config)
        assert result is True
        
        # éªŒè¯é…ç½®å·²æ›´æ–°
        updated_config = manager.get_provider_config('test_provider')
        assert updated_config.api_key == 'new-test-key'
        assert updated_config.base_url == 'https://api.test.com'
    
    def test_config_persistence(self):
        """æµ‹è¯•é…ç½®æŒä¹…åŒ–"""
        manager = DynamicConfigManager()
        
        # æ¨¡æ‹Ÿé…ç½®ä¿å­˜
        with patch('builtins.open', mock_open()) as mock_file:
            manager.save_config_to_file()
            mock_file.assert_called_once()
    
    def test_config_validation(self):
        """æµ‹è¯•é…ç½®éªŒè¯"""
        manager = DynamicConfigManager()
        
        # æœ‰æ•ˆé…ç½®
        valid_config = {
            'api_key': 'valid-key',
            'model_name': 'test-model'
        }
        assert manager.validate_config(valid_config) is True
        
        # æ— æ•ˆé…ç½®
        invalid_config = {
            'api_key': '',  # ç©ºAPIå¯†é’¥
            'model_name': 'test-model'
        }
        assert manager.validate_config(invalid_config) is False
```

### 3. å·¥å…·å‡½æ•°æµ‹è¯•

```python
# tests/unit/test_utils.py
import pytest
from core.writer_utils import KeyPointMsg, process_text, format_output
from core.parser_utils import parse_yaml_content, extract_sections

class TestUtilFunctions:
    def test_text_processing(self):
        """æµ‹è¯•æ–‡æœ¬å¤„ç†å‡½æ•°"""
        # æµ‹è¯•åŸºæœ¬æ–‡æœ¬å¤„ç†
        input_text = "  è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬  \n\n  "
        result = process_text(input_text)
        assert result == "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬"
        
        # æµ‹è¯•ç©ºæ–‡æœ¬
        assert process_text("") == ""
        assert process_text(None) == ""
        
        # æµ‹è¯•ç‰¹æ®Šå­—ç¬¦
        special_text = "æµ‹è¯•\t\n\rç‰¹æ®Šå­—ç¬¦"
        result = process_text(special_text)
        assert "\t" not in result
        assert "\n" not in result
        assert "\r" not in result
    
    def test_yaml_parsing(self):
        """æµ‹è¯•YAMLè§£æ"""
        yaml_content = """
        title: æµ‹è¯•æ ‡é¢˜
        chapters:
          - name: ç¬¬ä¸€ç« 
            content: ç¬¬ä¸€ç« å†…å®¹
          - name: ç¬¬äºŒç« 
            content: ç¬¬äºŒç« å†…å®¹
        """
        
        result = parse_yaml_content(yaml_content)
        assert result['title'] == 'æµ‹è¯•æ ‡é¢˜'
        assert len(result['chapters']) == 2
        assert result['chapters'][0]['name'] == 'ç¬¬ä¸€ç« '
    
    def test_section_extraction(self):
        """æµ‹è¯•æ®µè½æå–"""
        text = """
        # ç¬¬ä¸€ç« 
        è¿™æ˜¯ç¬¬ä¸€ç« çš„å†…å®¹
        
        # ç¬¬äºŒç« 
        è¿™æ˜¯ç¬¬äºŒç« çš„å†…å®¹
        """
        
        sections = extract_sections(text)
        assert len(sections) == 2
        assert sections[0]['title'] == 'ç¬¬ä¸€ç« '
        assert sections[1]['title'] == 'ç¬¬äºŒç« '
    
    def test_format_output(self):
        """æµ‹è¯•è¾“å‡ºæ ¼å¼åŒ–"""
        data = {
            'title': 'æµ‹è¯•æ ‡é¢˜',
            'content': 'æµ‹è¯•å†…å®¹',
            'metadata': {'author': 'æµ‹è¯•ä½œè€…'}
        }
        
        # æµ‹è¯•JSONæ ¼å¼
        json_output = format_output(data, 'json')
        assert '"title"' in json_output
        assert '"æµ‹è¯•æ ‡é¢˜"' in json_output
        
        # æµ‹è¯•YAMLæ ¼å¼
        yaml_output = format_output(data, 'yaml')
        assert 'title: æµ‹è¯•æ ‡é¢˜' in yaml_output
        
        # æµ‹è¯•çº¯æ–‡æœ¬æ ¼å¼
        text_output = format_output(data, 'text')
        assert 'æµ‹è¯•æ ‡é¢˜' in text_output
        assert 'æµ‹è¯•å†…å®¹' in text_output
```

## é›†æˆæµ‹è¯•

### 1. APIé›†æˆæµ‹è¯•

```python
# tests/integration/test_api.py
import pytest
import requests
import json
from unittest.mock import patch

class TestAPIIntegration:
    def test_health_endpoint(self, client):
        """æµ‹è¯•å¥åº·æ£€æŸ¥ç«¯ç‚¹"""
        response = client.get('/health')
        assert response.status_code == 200
        data = response.get_json()
        assert data['status'] == 'healthy'
    
    def test_write_endpoint(self, client):
        """æµ‹è¯•å†™ä½œç«¯ç‚¹"""
        request_data = {
            'writer_mode': 'draft',
            'chunk_list': [['ç« èŠ‚1', 'åŸå§‹å†…å®¹1']],
            'chunk_span': [0, 1],
            'prompt_content': 'æµ‹è¯•æç¤ºè¯',
            'x_chunk_length': 100,
            'y_chunk_length': 200,
            'main_model': 'test/model',
            'sub_model': 'test/model',
            'global_context': 'æµ‹è¯•ä¸Šä¸‹æ–‡',
            'only_prompt': False,
            'settings': {
                'MAX_THREAD_NUM': 1
            }
        }
        
        with patch('backend.backend_utils.get_model_config_from_provider_model') as mock_config:
            mock_config.return_value = {
                'model': 'test-model',
                'api_key': 'test-key',
                'max_tokens': 1000
            }
            
            response = client.post('/write', 
                                 data=json.dumps(request_data),
                                 content_type='application/json')
            
            assert response.status_code == 200
            assert response.mimetype == 'text/event-stream'
    
    def test_prompts_endpoint(self, client):
        """æµ‹è¯•æç¤ºè¯ç«¯ç‚¹"""
        response = client.get('/prompts')
        assert response.status_code == 200
        data = response.get_json()
        assert 'outline' in data
        assert 'plot' in data
        assert 'draft' in data
    
    def test_settings_endpoint(self, client):
        """æµ‹è¯•è®¾ç½®ç«¯ç‚¹"""
        response = client.get('/setting')
        assert response.status_code == 200
        data = response.get_json()
        assert 'models' in data
        assert 'MAIN_MODEL' in data
        assert 'SUB_MODEL' in data
```

### 2. æ•°æ®åº“é›†æˆæµ‹è¯•

```python
# tests/integration/test_database.py
import pytest
from unittest.mock import patch, Mock
from llm_api.mongodb_cache import llm_api_cache
from llm_api.mongodb_cost import CostTracker

class TestDatabaseIntegration:
    @pytest.fixture
    def mock_mongodb(self):
        """Mock MongoDBè¿æ¥"""
        with patch('pymongo.MongoClient') as mock_client:
            mock_db = Mock()
            mock_collection = Mock()
            mock_client.return_value = mock_db
            mock_db.__getitem__.return_value = mock_collection
            yield mock_client, mock_db, mock_collection
    
    def test_cache_operations(self, mock_mongodb):
        """æµ‹è¯•ç¼“å­˜æ“ä½œ"""
        mock_client, mock_db, mock_collection = mock_mongodb
        
        # æµ‹è¯•ç¼“å­˜å­˜å‚¨
        cache_key = "test_key"
        cache_value = {"result": "test_result"}
        
        # æ¨¡æ‹Ÿç¼“å­˜æŸ¥æ‰¾ï¼ˆæœªå‘½ä¸­ï¼‰
        mock_collection.find_one.return_value = None
        
        # æ¨¡æ‹Ÿç¼“å­˜å­˜å‚¨
        mock_collection.insert_one.return_value = Mock(inserted_id="test_id")
        
        # æ‰§è¡Œç¼“å­˜æ“ä½œ
        @llm_api_cache()
        def test_function():
            return cache_value
        
        result = test_function()
        assert result == cache_value
    
    def test_cost_tracking(self, mock_mongodb):
        """æµ‹è¯•æˆæœ¬è¿½è¸ª"""
        mock_client, mock_db, mock_collection = mock_mongodb
        
        tracker = CostTracker()
        
        # æµ‹è¯•æˆæœ¬è®°å½•
        cost_data = {
            'model': 'test-model',
            'tokens': 1000,
            'cost': 0.01,
            'timestamp': '2023-01-01T00:00:00'
        }
        
        mock_collection.insert_one.return_value = Mock(inserted_id="cost_id")
        
        result = tracker.record_cost(cost_data)
        assert result is not None
        mock_collection.insert_one.assert_called_once()
    
    def test_database_connection_error(self):
        """æµ‹è¯•æ•°æ®åº“è¿æ¥é”™è¯¯"""
        with patch('pymongo.MongoClient') as mock_client:
            mock_client.side_effect = Exception("Connection failed")
            
            # æµ‹è¯•è¿æ¥å¤±è´¥æ—¶çš„å¤„ç†
            @llm_api_cache()
            def test_function():
                return {"result": "test"}
            
            # åº”è¯¥èƒ½å¤Ÿæ­£å¸¸è¿è¡Œï¼ˆä¸ä½¿ç”¨ç¼“å­˜ï¼‰
            result = test_function()
            assert result == {"result": "test"}
```

### 3. å·¥ä½œæµé›†æˆæµ‹è¯•

```python
# tests/integration/test_workflow.py
import pytest
from unittest.mock import patch, Mock
from core.draft_writer import DraftWriter
from core.plot_writer import PlotWriter
from core.outline_writer import OutlineWriter

class TestWorkflowIntegration:
    def test_complete_writing_workflow(self):
        """æµ‹è¯•å®Œæ•´å†™ä½œæµç¨‹"""
        # å‡†å¤‡æµ‹è¯•æ•°æ®
        chunks = [
            ("ç« èŠ‚1", "åŸå§‹å†…å®¹1"),
            ("ç« èŠ‚2", "åŸå§‹å†…å®¹2")
        ]
        
        # Mockæ¨¡å‹é…ç½®
        mock_config = {
            'model': 'test-model',
            'api_key': 'test-key',
            'max_tokens': 1000
        }
        
        with patch('llm_api.stream_chat') as mock_stream:
            # è®¾ç½®mockè¿”å›å€¼
            mock_stream.return_value = iter([
                Mock(content="æ”¹è¿›åçš„å†…å®¹1"),
                Mock(content="æ”¹è¿›åçš„å†…å®¹2")
            ])
            
            # åˆ›å»ºå†™ä½œå™¨
            writer = DraftWriter(
                xy_pairs=chunks,
                model=mock_config,
                sub_model=mock_config
            )
            
            # æ‰§è¡Œå†™ä½œ
            result = writer.write("æ”¹è¿›è¿™äº›å†…å®¹")
            
            # éªŒè¯ç»“æœ
            assert result is not None
            mock_stream.assert_called()
    
    def test_multi_stage_workflow(self):
        """æµ‹è¯•å¤šé˜¶æ®µå·¥ä½œæµ"""
        # é˜¶æ®µ1ï¼šå¤§çº²ç”Ÿæˆ
        outline_writer = OutlineWriter()
        
        with patch('llm_api.stream_chat') as mock_stream:
            mock_stream.return_value = iter([
                Mock(content="ç”Ÿæˆçš„å¤§çº²")
            ])
            
            outline = outline_writer.generate_outline("ç§‘å¹»å°è¯´")
            assert outline is not None
        
        # é˜¶æ®µ2ï¼šå‰§æƒ…ç”Ÿæˆ
        plot_writer = PlotWriter()
        
        with patch('llm_api.stream_chat') as mock_stream:
            mock_stream.return_value = iter([
                Mock(content="ç”Ÿæˆçš„å‰§æƒ…")
            ])
            
            plot = plot_writer.generate_plot(outline)
            assert plot is not None
        
        # é˜¶æ®µ3ï¼šæ­£æ–‡ç”Ÿæˆ
        draft_writer = DraftWriter()
        
        with patch('llm_api.stream_chat') as mock_stream:
            mock_stream.return_value = iter([
                Mock(content="ç”Ÿæˆçš„æ­£æ–‡")
            ])
            
            draft = draft_writer.generate_draft(plot)
            assert draft is not None
```

## APIæµ‹è¯•

### 1. å¤–éƒ¨APIæµ‹è¯•

```python
# tests/api/test_external_apis.py
import pytest
import requests
import responses
from llm_api.openai_api import stream_chat_with_gpt
from llm_api.doubao_api import stream_chat_with_doubao
from llm_api.zhipuai_api import stream_chat_with_zhipuai

class TestExternalAPIs:
    @responses.activate
    def test_openai_api_success(self):
        """æµ‹è¯•OpenAI APIæˆåŠŸè°ƒç”¨"""
        # æ¨¡æ‹ŸAPIå“åº”
        responses.add(
            responses.POST,
            'https://api.openai.com/v1/chat/completions',
            json={
                "choices": [
                    {
                        "delta": {"content": "æµ‹è¯•å“åº”"},
                        "finish_reason": None
                    }
                ]
            },
            status=200
        )
        
        messages = [{"role": "user", "content": "æµ‹è¯•æ¶ˆæ¯"}]
        
        # æµ‹è¯•APIè°ƒç”¨
        result = list(stream_chat_with_gpt(
            messages=messages,
            api_key="test-key",
            model="gpt-3.5-turbo"
        ))
        
        assert len(result) > 0
        assert result[-1][-1]['content'] == "æµ‹è¯•å“åº”"
    
    @responses.activate
    def test_openai_api_error(self):
        """æµ‹è¯•OpenAI APIé”™è¯¯å¤„ç†"""
        # æ¨¡æ‹ŸAPIé”™è¯¯å“åº”
        responses.add(
            responses.POST,
            'https://api.openai.com/v1/chat/completions',
            json={"error": {"message": "Invalid API key"}},
            status=401
        )
        
        messages = [{"role": "user", "content": "æµ‹è¯•æ¶ˆæ¯"}]
        
        # æµ‹è¯•é”™è¯¯å¤„ç†
        with pytest.raises(Exception) as exc_info:
            list(stream_chat_with_gpt(
                messages=messages,
                api_key="invalid-key",
                model="gpt-3.5-turbo"
            ))
        
        assert "Invalid API key" in str(exc_info.value)
    
    def test_api_timeout(self):
        """æµ‹è¯•APIè¶…æ—¶"""
        messages = [{"role": "user", "content": "æµ‹è¯•æ¶ˆæ¯"}]
        
        with pytest.raises(Exception) as exc_info:
            list(stream_chat_with_gpt(
                messages=messages,
                api_key="test-key",
                model="gpt-3.5-turbo",
                timeout=0.001  # å¾ˆçŸ­çš„è¶…æ—¶æ—¶é—´
            ))
        
        assert "timeout" in str(exc_info.value).lower()
    
    @responses.activate
    def test_lmstudio_api(self):
        """æµ‹è¯•LM Studioæœ¬åœ°API"""
        # æ¨¡æ‹Ÿæœ¬åœ°APIå“åº”
        responses.add(
            responses.POST,
            'http://localhost:1234/v1/chat/completions',
            json={
                "choices": [
                    {
                        "delta": {"content": "æœ¬åœ°æ¨¡å‹å“åº”"},
                        "finish_reason": None
                    }
                ]
            },
            status=200
        )
        
        messages = [{"role": "user", "content": "æµ‹è¯•æ¶ˆæ¯"}]
        
        result = list(stream_chat_with_gpt(
            messages=messages,
            api_key="lm-studio",
            base_url="http://localhost:1234/v1",
            model="local-model"
        ))
        
        assert len(result) > 0
        assert result[-1][-1]['content'] == "æœ¬åœ°æ¨¡å‹å“åº”"
```

### 2. APIæ€§èƒ½æµ‹è¯•

```python
# tests/api/test_api_performance.py
import pytest
import time
import concurrent.futures
from llm_api.openai_api import stream_chat_with_gpt

class TestAPIPerformance:
    def test_api_response_time(self):
        """æµ‹è¯•APIå“åº”æ—¶é—´"""
        messages = [{"role": "user", "content": "çŸ­æ¶ˆæ¯æµ‹è¯•"}]
        
        start_time = time.time()
        
        with pytest.raises(Exception):  # æ²¡æœ‰çœŸå®APIå¯†é’¥ä¼šå¤±è´¥
            list(stream_chat_with_gpt(
                messages=messages,
                api_key="test-key",
                model="gpt-3.5-turbo"
            ))
        
        end_time = time.time()
        response_time = end_time - start_time
        
        # å³ä½¿å¤±è´¥ï¼Œä¹Ÿåº”è¯¥åœ¨åˆç†æ—¶é—´å†…è¿”å›
        assert response_time < 30  # 30ç§’å†…åº”è¯¥è¿”å›
    
    def test_concurrent_api_calls(self):
        """æµ‹è¯•å¹¶å‘APIè°ƒç”¨"""
        messages = [{"role": "user", "content": "å¹¶å‘æµ‹è¯•"}]
        
        def make_api_call():
            try:
                return list(stream_chat_with_gpt(
                    messages=messages,
                    api_key="test-key",
                    model="gpt-3.5-turbo"
                ))
            except Exception:
                return None
        
        # æµ‹è¯•5ä¸ªå¹¶å‘è°ƒç”¨
        with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
            start_time = time.time()
            futures = [executor.submit(make_api_call) for _ in range(5)]
            results = [future.result() for future in futures]
            end_time = time.time()
        
        # éªŒè¯å¹¶å‘æ€§èƒ½
        total_time = end_time - start_time
        assert total_time < 60  # 5ä¸ªå¹¶å‘è°ƒç”¨åº”è¯¥åœ¨60ç§’å†…å®Œæˆ
        assert len(results) == 5
    
    def test_memory_usage(self):
        """æµ‹è¯•å†…å­˜ä½¿ç”¨"""
        import psutil
        import os
        
        process = psutil.Process(os.getpid())
        initial_memory = process.memory_info().rss
        
        # æ‰§è¡Œå¤šæ¬¡APIè°ƒç”¨
        messages = [{"role": "user", "content": "å†…å­˜æµ‹è¯•"}]
        for _ in range(10):
            try:
                list(stream_chat_with_gpt(
                    messages=messages,
                    api_key="test-key",
                    model="gpt-3.5-turbo"
                ))
            except Exception:
                pass
        
        final_memory = process.memory_info().rss
        memory_increase = final_memory - initial_memory
        
        # å†…å­˜å¢é•¿åº”è¯¥æ§åˆ¶åœ¨åˆç†èŒƒå›´å†…ï¼ˆ50MBï¼‰
        assert memory_increase < 50 * 1024 * 1024
```

## æ€§èƒ½æµ‹è¯•

### 1. è´Ÿè½½æµ‹è¯•

```python
# tests/performance/test_load.py
import pytest
import time
import threading
from locust import HttpUser, task, between

class WebsiteUser(HttpUser):
    wait_time = between(1, 3)
    
    def on_start(self):
        """æµ‹è¯•å¼€å§‹æ—¶çš„åˆå§‹åŒ–"""
        self.client.headers.update({
            'Content-Type': 'application/json'
        })
    
    @task(3)
    def test_health_endpoint(self):
        """æµ‹è¯•å¥åº·æ£€æŸ¥ç«¯ç‚¹è´Ÿè½½"""
        self.client.get("/health")
    
    @task(1)
    def test_prompts_endpoint(self):
        """æµ‹è¯•æç¤ºè¯ç«¯ç‚¹è´Ÿè½½"""
        self.client.get("/prompts")
    
    @task(1)
    def test_settings_endpoint(self):
        """æµ‹è¯•è®¾ç½®ç«¯ç‚¹è´Ÿè½½"""
        self.client.get("/setting")

class TestPerformance:
    def test_response_time_under_load(self):
        """æµ‹è¯•è´Ÿè½½ä¸‹çš„å“åº”æ—¶é—´"""
        import requests
        import threading
        
        results = []
        
        def make_request():
            start_time = time.time()
            try:
                response = requests.get('http://localhost:7869/health')
                end_time = time.time()
                results.append({
                    'status_code': response.status_code,
                    'response_time': end_time - start_time
                })
            except Exception as e:
                results.append({'error': str(e)})
        
        # åˆ›å»º10ä¸ªå¹¶å‘çº¿ç¨‹
        threads = []
        for _ in range(10):
            thread = threading.Thread(target=make_request)
            threads.append(thread)
            thread.start()
        
        # ç­‰å¾…æ‰€æœ‰çº¿ç¨‹å®Œæˆ
        for thread in threads:
            thread.join()
        
        # åˆ†æç»“æœ
        successful_requests = [r for r in results if 'status_code' in r]
        if successful_requests:
            avg_response_time = sum(r['response_time'] for r in successful_requests) / len(successful_requests)
            assert avg_response_time < 5.0  # å¹³å‡å“åº”æ—¶é—´åº”å°äº5ç§’
            assert all(r['status_code'] == 200 for r in successful_requests)
    
    def test_memory_usage_under_load(self):
        """æµ‹è¯•è´Ÿè½½ä¸‹çš„å†…å­˜ä½¿ç”¨"""
        import psutil
        import os
        
        process = psutil.Process(os.getpid())
        initial_memory = process.memory_info().rss
        
        # æ¨¡æ‹Ÿé«˜è´Ÿè½½
        for i in range(100):
            # åˆ›å»ºå¤§é‡å¯¹è±¡
            data = [f"æµ‹è¯•æ•°æ®{j}" for j in range(1000)]
            
            # æ£€æŸ¥å†…å­˜å¢é•¿
            if i % 10 == 0:
                current_memory = process.memory_info().rss
                memory_increase = current_memory - initial_memory
                
                # å†…å­˜å¢é•¿ä¸åº”è¶…è¿‡100MB
                assert memory_increase < 100 * 1024 * 1024
```

### 2. åŸºå‡†æµ‹è¯•

```python
# tests/performance/test_benchmarks.py
import pytest
import time
from core.writer_utils import process_text
from core.parser_utils import parse_yaml_content

class TestBenchmarks:
    def test_text_processing_benchmark(self, benchmark):
        """æ–‡æœ¬å¤„ç†æ€§èƒ½åŸºå‡†æµ‹è¯•"""
        test_text = "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬ " * 1000
        
        result = benchmark(process_text, test_text)
        
        assert result is not None
        assert len(result) > 0
    
    def test_yaml_parsing_benchmark(self, benchmark):
        """YAMLè§£ææ€§èƒ½åŸºå‡†æµ‹è¯•"""
        yaml_content = """
        title: æµ‹è¯•æ ‡é¢˜
        chapters:
        """ + "\n".join([f"  - name: ç¬¬{i}ç« \n    content: ç¬¬{i}ç« å†…å®¹" for i in range(100)])
        
        result = benchmark(parse_yaml_content, yaml_content)
        
        assert result is not None
        assert 'title' in result
        assert len(result['chapters']) == 100
    
    def test_large_data_processing(self):
        """å¤§æ•°æ®å¤„ç†æ€§èƒ½æµ‹è¯•"""
        # åˆ›å»ºå¤§é‡æµ‹è¯•æ•°æ®
        large_data = ["æµ‹è¯•æ•°æ®è¡Œ" + str(i) for i in range(10000)]
        
        start_time = time.time()
        
        # å¤„ç†å¤§é‡æ•°æ®
        processed_data = [process_text(line) for line in large_data]
        
        end_time = time.time()
        processing_time = end_time - start_time
        
        # éªŒè¯å¤„ç†ç»“æœ
        assert len(processed_data) == 10000
        assert processing_time < 10.0  # å¤„ç†10000æ¡æ•°æ®åº”åœ¨10ç§’å†…å®Œæˆ
        
        # éªŒè¯å¤„ç†é€Ÿåº¦
        items_per_second = len(large_data) / processing_time
        assert items_per_second > 1000  # æ¯ç§’å¤„ç†è¶…è¿‡1000æ¡
```

## å‰ç«¯æµ‹è¯•

### 1. UIè‡ªåŠ¨åŒ–æµ‹è¯•

```python
# tests/frontend/test_ui.py
import pytest
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

class TestUI:
    @pytest.fixture
    def driver(self):
        """æµè§ˆå™¨é©±åŠ¨fixture"""
        options = webdriver.ChromeOptions()
        options.add_argument('--headless')  # æ— å¤´æ¨¡å¼
        driver = webdriver.Chrome(options=options)
        yield driver
        driver.quit()
    
    def test_page_load(self, driver):
        """æµ‹è¯•é¡µé¢åŠ è½½"""
        driver.get('http://localhost:8099')
        
        # ç­‰å¾…é¡µé¢åŠ è½½
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.TAG_NAME, "body"))
        )
        
        # éªŒè¯æ ‡é¢˜
        assert "Long-Novel-GPT" in driver.title
    
    def test_settings_modal(self, driver):
        """æµ‹è¯•è®¾ç½®æ¨¡æ€æ¡†"""
        driver.get('http://localhost:8099')
        
        # ç‚¹å‡»è®¾ç½®æŒ‰é’®
        settings_btn = WebDriverWait(driver, 10).until(
            EC.element_to_be_clickable((By.CLASS_NAME, "settings-btn"))
        )
        settings_btn.click()
        
        # éªŒè¯æ¨¡æ€æ¡†å‡ºç°
        modal = WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.CLASS_NAME, "modal"))
        )
        assert modal.is_displayed()
    
    def test_provider_selection(self, driver):
        """æµ‹è¯•æä¾›å•†é€‰æ‹©"""
        driver.get('http://localhost:8099')
        
        # æ‰“å¼€è®¾ç½®
        settings_btn = driver.find_element(By.CLASS_NAME, "settings-btn")
        settings_btn.click()
        
        # é€‰æ‹©æä¾›å•†
        provider_select = WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.CLASS_NAME, "provider-select"))
        )
        
        # éªŒè¯é€‰é¡¹å­˜åœ¨
        options = provider_select.find_elements(By.TAG_NAME, "option")
        assert len(options) > 0
        
        # éªŒè¯é»˜è®¤é€‰æ‹©
        selected_option = provider_select.find_element(By.CSS_SELECTOR, "option:checked")
        assert selected_option is not None
    
    def test_text_generation(self, driver):
        """æµ‹è¯•æ–‡æœ¬ç”ŸæˆåŠŸèƒ½"""
        driver.get('http://localhost:8099')
        
        # è¾“å…¥æç¤ºè¯
        prompt_input = WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.CLASS_NAME, "prompt-input"))
        )
        prompt_input.send_keys("æµ‹è¯•æç¤ºè¯")
        
        # ç‚¹å‡»ç”ŸæˆæŒ‰é’®
        generate_btn = driver.find_element(By.CLASS_NAME, "generate-btn")
        generate_btn.click()
        
        # éªŒè¯ç”Ÿæˆè¿‡ç¨‹ï¼ˆå¯èƒ½éœ€è¦ç­‰å¾…ï¼‰
        # è¿™é‡Œåº”è¯¥æ·»åŠ å¯¹ç”Ÿæˆç»“æœçš„éªŒè¯
        # ç”±äºå®é™…ç”Ÿæˆéœ€è¦APIè°ƒç”¨ï¼Œè¿™é‡ŒåªéªŒè¯UIçŠ¶æ€å˜åŒ–
        
        # éªŒè¯æŒ‰é’®çŠ¶æ€å˜åŒ–
        WebDriverWait(driver, 5).until(
            lambda d: generate_btn.text != "å¼€å§‹ç”Ÿæˆ"
        )
```

### 2. å‰ç«¯APIé›†æˆæµ‹è¯•

```python
# tests/frontend/test_api_integration.py
import pytest
import requests
import json

class TestFrontendAPIIntegration:
    def test_frontend_backend_communication(self):
        """æµ‹è¯•å‰ç«¯ä¸åç«¯çš„é€šä¿¡"""
        # æ¨¡æ‹Ÿå‰ç«¯å‘é€çš„è¯·æ±‚
        frontend_request = {
            'writer_mode': 'draft',
            'chunk_list': [['æµ‹è¯•ç« èŠ‚', 'æµ‹è¯•å†…å®¹']],
            'prompt_content': 'æµ‹è¯•æç¤ºè¯',
            'main_model': 'test/model',
            'sub_model': 'test/model',
            'settings': {
                'MAX_THREAD_NUM': 1
            }
        }
        
        # å‘é€è¯·æ±‚åˆ°åç«¯
        response = requests.post(
            'http://localhost:7869/write',
            json=frontend_request,
            headers={'Content-Type': 'application/json'}
        )
        
        # éªŒè¯å“åº”
        assert response.status_code == 200
        assert response.headers['Content-Type'] == 'text/event-stream; charset=utf-8'
    
    def test_settings_synchronization(self):
        """æµ‹è¯•è®¾ç½®åŒæ­¥"""
        # è·å–åç«¯è®¾ç½®
        response = requests.get('http://localhost:7869/setting')
        assert response.status_code == 200
        
        settings = response.json()
        assert 'models' in settings
        assert 'MAIN_MODEL' in settings
        assert 'SUB_MODEL' in settings
        
        # éªŒè¯è®¾ç½®æ ¼å¼
        assert isinstance(settings['models'], dict)
        assert isinstance(settings['MAIN_MODEL'], str)
        assert isinstance(settings['SUB_MODEL'], str)
    
    def test_error_handling(self):
        """æµ‹è¯•é”™è¯¯å¤„ç†"""
        # å‘é€æ— æ•ˆè¯·æ±‚
        invalid_request = {
            'writer_mode': 'invalid_mode',
            'chunk_list': [],
            'prompt_content': ''
        }
        
        response = requests.post(
            'http://localhost:7869/write',
            json=invalid_request,
            headers={'Content-Type': 'application/json'}
        )
        
        # éªŒè¯é”™è¯¯å“åº”
        assert response.status_code in [400, 422, 500]
```

## è‡ªåŠ¨åŒ–æµ‹è¯•

### 1. CI/CDæµ‹è¯•é…ç½®

```yaml
# .github/workflows/test.yml
name: Test Suite

on:
  push:
    branches: [ main, dev ]
  pull_request:
    branches: [ main, dev ]

jobs:
  test:
    runs-on: ubuntu-latest
    
    strategy:
      matrix:
        python-version: [3.8, 3.9, 3.10]
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v3
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-test.txt
    
    - name: Run unit tests
      run: |
        pytest tests/unit/ -v --cov=core --cov=llm_api
    
    - name: Run integration tests
      run: |
        pytest tests/integration/ -v
    
    - name: Run performance tests
      run: |
        pytest tests/performance/ -v --benchmark-only
    
    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        flags: unittests
```

### 2. æµ‹è¯•è„šæœ¬

```python
# scripts/run_tests.py
#!/usr/bin/env python3
"""æµ‹è¯•è¿è¡Œè„šæœ¬"""

import subprocess
import sys
import os
import argparse

def run_unit_tests():
    """è¿è¡Œå•å…ƒæµ‹è¯•"""
    print("Running unit tests...")
    result = subprocess.run([
        'pytest', 'tests/unit/', '-v', 
        '--cov=core', '--cov=llm_api',
        '--cov-report=html', '--cov-report=term-missing'
    ])
    return result.returncode == 0

def run_integration_tests():
    """è¿è¡Œé›†æˆæµ‹è¯•"""
    print("Running integration tests...")
    result = subprocess.run([
        'pytest', 'tests/integration/', '-v'
    ])
    return result.returncode == 0

def run_performance_tests():
    """è¿è¡Œæ€§èƒ½æµ‹è¯•"""
    print("Running performance tests...")
    result = subprocess.run([
        'pytest', 'tests/performance/', '-v', '--benchmark-only'
    ])
    return result.returncode == 0

def run_all_tests():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("Running all tests...")
    result = subprocess.run([
        'pytest', 'tests/', '-v',
        '--cov=core', '--cov=llm_api',
        '--cov-report=html', '--cov-report=term-missing'
    ])
    return result.returncode == 0

def main():
    parser = argparse.ArgumentParser(description='Run tests')
    parser.add_argument('--unit', action='store_true', help='Run unit tests')
    parser.add_argument('--integration', action='store_true', help='Run integration tests')
    parser.add_argument('--performance', action='store_true', help='Run performance tests')
    parser.add_argument('--all', action='store_true', help='Run all tests')
    
    args = parser.parse_args()
    
    if args.unit:
        success = run_unit_tests()
    elif args.integration:
        success = run_integration_tests()
    elif args.performance:
        success = run_performance_tests()
    elif args.all:
        success = run_all_tests()
    else:
        print("è¯·æŒ‡å®šæµ‹è¯•ç±»å‹: --unit, --integration, --performance, æˆ– --all")
        sys.exit(1)
    
    if success:
        print("âœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡")
        sys.exit(0)
    else:
        print("âŒ æµ‹è¯•å¤±è´¥")
        sys.exit(1)

if __name__ == '__main__':
    main()
```

## æµ‹è¯•æ•°æ®ç®¡ç†

### 1. æµ‹è¯•æ•°æ®ç”Ÿæˆ

```python
# tests/utils/test_data_generator.py
import json
import random
from datetime import datetime, timedelta

class TestDataGenerator:
    def __init__(self):
        self.fake_names = ["å¼ ä¸‰", "æå››", "ç‹äº”", "èµµå…­", "é’±ä¸ƒ"]
        self.fake_titles = ["æµ‹è¯•ç« èŠ‚", "ç¤ºä¾‹å†…å®¹", "æ ·æœ¬æ–‡æœ¬", "æ¨¡æ‹Ÿæ•°æ®", "è™šæ‹Ÿåœºæ™¯"]
    
    def generate_chunk_list(self, count=5):
        """ç”Ÿæˆæµ‹è¯•æ–‡æœ¬å—åˆ—è¡¨"""
        return [
            [f"ç¬¬{i+1}ç« ", f"è¿™æ˜¯ç¬¬{i+1}ç« çš„å†…å®¹ï¼ŒåŒ…å«ä¸€äº›æµ‹è¯•æ–‡æœ¬ã€‚"]
            for i in range(count)
        ]
    
    def generate_novel_data(self):
        """ç”Ÿæˆæµ‹è¯•å°è¯´æ•°æ®"""
        return {
            "title": random.choice(self.fake_titles),
            "author": random.choice(self.fake_names),
            "chapters": self.generate_chunk_list(10),
            "metadata": {
                "created_at": datetime.now().isoformat(),
                "word_count": random.randint(50000, 200000),
                "genre": random.choice(["ç§‘å¹»", "å¥‡å¹»", "è¨€æƒ…", "æ‚¬ç–‘"])
            }
        }
    
    def generate_api_response(self, content_type="text"):
        """ç”ŸæˆAPIå“åº”æ•°æ®"""
        if content_type == "text":
            return {
                "choices": [
                    {
                        "delta": {"content": "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•å“åº”å†…å®¹"},
                        "finish_reason": None
                    }
                ],
                "model": "test-model",
                "usage": {
                    "prompt_tokens": 10,
                    "completion_tokens": 20,
                    "total_tokens": 30
                }
            }
        elif content_type == "json":
            return {
                "choices": [
                    {
                        "delta": {"content": json.dumps({"result": "æµ‹è¯•ç»“æœ"})},
                        "finish_reason": None
                    }
                ]
            }
    
    def generate_config_data(self):
        """ç”Ÿæˆé…ç½®æµ‹è¯•æ•°æ®"""
        return {
            "providers": {
                "test_provider": {
                    "api_key": "test-key-123",
                    "base_url": "https://api.test.com/v1",
                    "model_name": "test-model",
                    "max_tokens": 1000
                }
            },
            "settings": {
                "max_thread_num": 5,
                "enable_cache": True,
                "debug_mode": False
            }
        }
    
    def save_test_data(self, data, filename):
        """ä¿å­˜æµ‹è¯•æ•°æ®åˆ°æ–‡ä»¶"""
        filepath = f"tests/fixtures/{filename}"
        os.makedirs(os.path.dirname(filepath), exist_ok=True)
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
```

### 2. æµ‹è¯•æ•°æ®æ¸…ç†

```python
# tests/utils/test_cleanup.py
import os
import tempfile
import shutil
from pathlib import Path

class TestCleanup:
    def __init__(self):
        self.temp_dirs = []
        self.temp_files = []
    
    def create_temp_dir(self, prefix="test_"):
        """åˆ›å»ºä¸´æ—¶ç›®å½•"""
        temp_dir = tempfile.mkdtemp(prefix=prefix)
        self.temp_dirs.append(temp_dir)
        return temp_dir
    
    def create_temp_file(self, suffix=".txt", content=""):
        """åˆ›å»ºä¸´æ—¶æ–‡ä»¶"""
        temp_file = tempfile.NamedTemporaryFile(
            mode='w', 
            suffix=suffix, 
            delete=False,
            encoding='utf-8'
        )
        temp_file.write(content)
        temp_file.close()
        self.temp_files.append(temp_file.name)
        return temp_file.name
    
    def cleanup(self):
        """æ¸…ç†æ‰€æœ‰ä¸´æ—¶æ–‡ä»¶å’Œç›®å½•"""
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        for temp_file in self.temp_files:
            try:
                os.unlink(temp_file)
            except FileNotFoundError:
                pass
        
        # æ¸…ç†ä¸´æ—¶ç›®å½•
        for temp_dir in self.temp_dirs:
            try:
                shutil.rmtree(temp_dir)
            except FileNotFoundError:
                pass
        
        # æ¸…ç†æµ‹è¯•ç”Ÿæˆçš„ç¼“å­˜æ–‡ä»¶
        cache_patterns = [
            "test_*.json",
            "test_*.yaml",
            "test_*.log",
            "__pycache__",
            "*.pyc"
        ]
        
        for pattern in cache_patterns:
            for file in Path(".").glob(f"**/{pattern}"):
                try:
                    if file.is_file():
                        file.unlink()
                    elif file.is_dir():
                        shutil.rmtree(file)
                except (FileNotFoundError, PermissionError):
                    pass
    
    def __enter__(self):
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        self.cleanup()
```

## æµ‹è¯•æŠ¥å‘Š

### 1. æµ‹è¯•æŠ¥å‘Šç”Ÿæˆ

```python
# scripts/generate_test_report.py
import json
import subprocess
import datetime
from pathlib import Path

def generate_coverage_report():
    """ç”Ÿæˆè¦†ç›–ç‡æŠ¥å‘Š"""
    result = subprocess.run([
        'pytest', '--cov=core', '--cov=llm_api', 
        '--cov-report=json', '--cov-report=html',
        'tests/'
    ], capture_output=True, text=True)
    
    # è¯»å–è¦†ç›–ç‡æ•°æ®
    try:
        with open('coverage.json', 'r') as f:
            coverage_data = json.load(f)
        
        return {
            'total_coverage': coverage_data['totals']['percent_covered'],
            'files_covered': len(coverage_data['files']),
            'lines_covered': coverage_data['totals']['covered_lines'],
            'lines_missing': coverage_data['totals']['missing_lines']
        }
    except FileNotFoundError:
        return None

def generate_test_summary():
    """ç”Ÿæˆæµ‹è¯•æ€»ç»“æŠ¥å‘Š"""
    result = subprocess.run([
        'pytest', '--json-report', '--json-report-file=test_report.json',
        'tests/'
    ], capture_output=True, text=True)
    
    try:
        with open('test_report.json', 'r') as f:
            test_data = json.load(f)
        
        return {
            'total_tests': test_data['summary']['total'],
            'passed': test_data['summary']['passed'],
            'failed': test_data['summary']['failed'],
            'errors': test_data['summary']['error'],
            'skipped': test_data['summary']['skipped'],
            'duration': test_data['duration']
        }
    except FileNotFoundError:
        return None

def generate_html_report():
    """ç”ŸæˆHTMLæµ‹è¯•æŠ¥å‘Š"""
    coverage_data = generate_coverage_report()
    test_summary = generate_test_summary()
    
    if not coverage_data or not test_summary:
        print("æ— æ³•ç”ŸæˆæŠ¥å‘Šï¼šç¼ºå°‘æµ‹è¯•æ•°æ®")
        return
    
    html_content = f"""
    <!DOCTYPE html>
    <html>
    <head>
        <title>Long-Novel-GPT æµ‹è¯•æŠ¥å‘Š</title>
        <style>
            body {{ font-family: Arial, sans-serif; margin: 20px; }}
            .summary {{ background: #f5f5f5; padding: 20px; margin: 20px 0; }}
            .metric {{ display: inline-block; margin: 10px; padding: 10px; background: white; border-radius: 5px; }}
            .passed {{ color: green; }}
            .failed {{ color: red; }}
            .coverage {{ color: blue; }}
        </style>
    </head>
    <body>
        <h1>Long-Novel-GPT æµ‹è¯•æŠ¥å‘Š</h1>
        <p>ç”Ÿæˆæ—¶é—´: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
        
        <div class="summary">
            <h2>æµ‹è¯•æ€»ç»“</h2>
            <div class="metric">
                <h3>æ€»æµ‹è¯•æ•°</h3>
                <p>{test_summary['total_tests']}</p>
            </div>
            <div class="metric passed">
                <h3>é€šè¿‡</h3>
                <p>{test_summary['passed']}</p>
            </div>
            <div class="metric failed">
                <h3>å¤±è´¥</h3>
                <p>{test_summary['failed']}</p>
            </div>
            <div class="metric">
                <h3>è·³è¿‡</h3>
                <p>{test_summary['skipped']}</p>
            </div>
            <div class="metric">
                <h3>ç”¨æ—¶</h3>
                <p>{test_summary['duration']:.2f}ç§’</p>
            </div>
        </div>
        
        <div class="summary">
            <h2>ä»£ç è¦†ç›–ç‡</h2>
            <div class="metric coverage">
                <h3>æ€»è¦†ç›–ç‡</h3>
                <p>{coverage_data['total_coverage']:.1f}%</p>
            </div>
            <div class="metric">
                <h3>è¦†ç›–æ–‡ä»¶æ•°</h3>
                <p>{coverage_data['files_covered']}</p>
            </div>
            <div class="metric">
                <h3>è¦†ç›–è¡Œæ•°</h3>
                <p>{coverage_data['lines_covered']}</p>
            </div>
            <div class="metric">
                <h3>æœªè¦†ç›–è¡Œæ•°</h3>
                <p>{coverage_data['lines_missing']}</p>
            </div>
        </div>
        
        <div class="summary">
            <h2>è¯¦ç»†æŠ¥å‘Š</h2>
            <p><a href="htmlcov/index.html">æŸ¥çœ‹è¯¦ç»†è¦†ç›–ç‡æŠ¥å‘Š</a></p>
        </div>
    </body>
    </html>
    """
    
    with open('test_report.html', 'w', encoding='utf-8') as f:
        f.write(html_content)
    
    print("æµ‹è¯•æŠ¥å‘Šå·²ç”Ÿæˆ: test_report.html")

if __name__ == '__main__':
    generate_html_report()
```

### 2. æŒç»­é›†æˆæŠ¥å‘Š

```yaml
# .github/workflows/test-report.yml
name: Test Report

on:
  push:
    branches: [ main ]
  schedule:
    - cron: '0 0 * * *'  # æ¯å¤©ç”ŸæˆæŠ¥å‘Š

jobs:
  test-report:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v3
      with:
        python-version: 3.9
    
    - name: Install dependencies
      run: |
        pip install -r requirements.txt
        pip install -r requirements-test.txt
    
    - name: Run tests with coverage
      run: |
        pytest --cov=core --cov=llm_api --cov-report=xml --cov-report=html
    
    - name: Generate test report
      run: |
        python scripts/generate_test_report.py
    
    - name: Upload test report
      uses: actions/upload-artifact@v3
      with:
        name: test-report
        path: |
          test_report.html
          htmlcov/
          coverage.xml
    
    - name: Comment PR
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          const coverage = fs.readFileSync('coverage.xml', 'utf8');
          const match = coverage.match(/line-rate="([^"]+)"/);
          const rate = match ? (parseFloat(match[1]) * 100).toFixed(1) : 'N/A';
          
          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: `## æµ‹è¯•æŠ¥å‘Š ğŸ“Š\n\nä»£ç è¦†ç›–ç‡: ${rate}%\n\n[æŸ¥çœ‹è¯¦ç»†æŠ¥å‘Š](https://github.com/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId})`
          });
```

## æœ€ä½³å®è·µ

### 1. æµ‹è¯•ç¼–å†™åŸåˆ™

- **å•ä¸€èŒè´£**ï¼šæ¯ä¸ªæµ‹è¯•åªéªŒè¯ä¸€ä¸ªåŠŸèƒ½ç‚¹
- **ç‹¬ç«‹æ€§**ï¼šæµ‹è¯•ä¹‹é—´ä¸åº”ç›¸äº’ä¾èµ–
- **å¯é‡å¤**ï¼šæµ‹è¯•ç»“æœåº”è¯¥ä¸€è‡´
- **å¿«é€Ÿæ‰§è¡Œ**ï¼šå•å…ƒæµ‹è¯•åº”è¯¥å¿«é€Ÿè¿è¡Œ
- **æ¸…æ™°å‘½å**ï¼šæµ‹è¯•åç§°åº”è¯¥æè¿°æµ‹è¯•çš„ç›®çš„

### 2. æµ‹è¯•ç»„ç»‡

```python
# å¥½çš„æµ‹è¯•ç»„ç»‡ç»“æ„
class TestUserAuthentication:
    def test_valid_credentials_should_return_success(self):
        """æœ‰æ•ˆå‡­æ®åº”è¯¥è¿”å›æˆåŠŸ"""
        pass
    
    def test_invalid_credentials_should_return_error(self):
        """æ— æ•ˆå‡­æ®åº”è¯¥è¿”å›é”™è¯¯"""
        pass
    
    def test_expired_token_should_require_refresh(self):
        """è¿‡æœŸä»¤ç‰Œåº”è¯¥éœ€è¦åˆ·æ–°"""
        pass
```

### 3. Mockä½¿ç”¨åŸåˆ™

```python
# åˆç†ä½¿ç”¨Mock
def test_api_call_with_mock():
    with patch('requests.post') as mock_post:
        mock_post.return_value.status_code = 200
        mock_post.return_value.json.return_value = {"success": True}
        
        result = api_function()
        
        assert result is True
        mock_post.assert_called_once()
```

## æ€»ç»“

æœ¬æµ‹è¯•æŒ‡å—æä¾›äº†Long-Novel-GPTé¡¹ç›®çš„å…¨é¢æµ‹è¯•ç­–ç•¥ï¼ŒåŒ…æ‹¬ï¼š

1. **å®Œæ•´çš„æµ‹è¯•æ¶æ„**ï¼šä»å•å…ƒæµ‹è¯•åˆ°ç«¯åˆ°ç«¯æµ‹è¯•
2. **å®ç”¨çš„æµ‹è¯•å·¥å…·**ï¼špytestã€mockã€seleniumç­‰
3. **è‡ªåŠ¨åŒ–æµ‹è¯•æµç¨‹**ï¼šCI/CDé›†æˆå’ŒæŠ¥å‘Šç”Ÿæˆ
4. **æ€§èƒ½æµ‹è¯•æ–¹æ³•**ï¼šè´Ÿè½½æµ‹è¯•å’ŒåŸºå‡†æµ‹è¯•
5. **æµ‹è¯•æ•°æ®ç®¡ç†**ï¼šç”Ÿæˆã€æ¸…ç†å’Œç»´æŠ¤æµ‹è¯•æ•°æ®

éµå¾ªè¿™äº›æµ‹è¯•å®è·µå¯ä»¥ç¡®ä¿ç³»ç»Ÿçš„ç¨³å®šæ€§å’Œå¯é æ€§ã€‚

æ›´å¤šè¯¦ç»†ä¿¡æ¯è¯·å‚è€ƒï¼š
- [è°ƒè¯•å’Œæ•…éšœæ’é™¤æŒ‡å—](DEBUGGING_GUIDE.md)
- [å¿«é€Ÿå¼€å‘æŒ‡å—](QUICK_DEVELOPMENT.md)
- [ç¯å¢ƒé…ç½®æŒ‡å—](ENVIRONMENT_SETUP.md)

---

*æœ¬æ–‡æ¡£ä¼šæ ¹æ®é¡¹ç›®å‘å±•æŒç»­æ›´æ–°ï¼Œå¦‚æœ‰ç–‘é—®è¯·æŸ¥çœ‹é¡¹ç›®GitHubæˆ–æäº¤Issueã€‚* 
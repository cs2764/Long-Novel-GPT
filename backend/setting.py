from flask import Blueprint, jsonify, request

setting_bp = Blueprint('setting', __name__)

@setting_bp.route('/setting', methods=['GET'])
def get_settings():
    """Get current settings and models"""
    from config import API_SETTINGS, DEFAULT_MAIN_MODEL, DEFAULT_SUB_MODEL, MAX_THREAD_NUM, MAX_NOVEL_SUMMARY_LENGTH
    
    # Get models grouped by provider
    models = {provider: config['available_models'] for provider, config in API_SETTINGS.items() if 'available_models' in config}
    
    # Combine all settings
    settings = {
        'models': models,
        'MAIN_MODEL': DEFAULT_MAIN_MODEL,
        'SUB_MODEL': DEFAULT_SUB_MODEL,
        'MAX_THREAD_NUM': MAX_THREAD_NUM,
        'MAX_NOVEL_SUMMARY_LENGTH': MAX_NOVEL_SUMMARY_LENGTH,
    }
    return jsonify(settings)

@setting_bp.route('/test_model', methods=['POST'])
def test_model():
    """Test if a model configuration works"""
    import traceback
    
    print(f"\n=== Testing Model via /test_model endpoint ===")
    
    try:
        data = request.get_json()
        provider_model = data.get('provider_model')
        
        print(f"Provider model: {provider_model}")
        
        if not provider_model:
            error_msg = "provider_modelå‚æ•°ä¸èƒ½ä¸ºç©º"
            print(f"âŒ {error_msg}")
            return jsonify({
                'success': False,
                'error': error_msg
            }), 400
        
        # ä½¿ç”¨åŠ¨æ€é…ç½®ç®¡ç†å™¨è·å–æ¨¡å‹é…ç½®
        from core.dynamic_config_manager import get_config_manager
        from llm_api import ModelConfig
        
        config_manager = get_config_manager()
        
        # è§£æ provider/model æ ¼å¼
        if '/' in provider_model:
            provider_name, model_name = provider_model.split('/', 1)
        else:
            # å¦‚æœæ²¡æœ‰ '/'ï¼Œå°è¯•æŸ¥æ‰¾å½“å‰æä¾›å•†
            provider_name = config_manager.get_current_provider()
            model_name = provider_model
            
        print(f"Provider: {provider_name}, Model: {model_name}")
        
        # è·å–æä¾›å•†é…ç½®
        provider_config = config_manager.get_provider_config(provider_name)
        if not provider_config:
            error_msg = f"æœªæ‰¾åˆ°æä¾›å•†é…ç½®: {provider_name}"
            print(f"âŒ {error_msg}")
            return jsonify({
                'success': False,
                'error': error_msg
            }), 400
            
        # æ„å»ºæ¨¡å‹é…ç½®
        model_config = ModelConfig(
            model=model_name,
            api_key=provider_config.api_key,
            max_tokens=100  # æµ‹è¯•æ—¶ä½¿ç”¨å°çš„tokenæ•°
        )
        
        # æ·»åŠ ç‰¹å®šæä¾›å•†çš„é¢å¤–é…ç½®
        if provider_config.base_url:
            model_config['base_url'] = provider_config.base_url
            
        print(f"Model config: {dict(model_config)}")
        
        from llm_api import test_stream_chat
        response = None
        test_content = ""
        
        print(f"ğŸ§ª Starting model test...")
        
        for msg in test_stream_chat(model_config):
            response = msg
            test_content += msg
            if len(test_content) > 100:  # é™åˆ¶æµ‹è¯•å“åº”é•¿åº¦
                break
            
        print(f"âœ… Model test completed successfully")
        print(f"Response: {test_content[:100]}...")
        
        return jsonify({
            'success': True,
            'response': test_content
        })
        
    except Exception as e:
        error_msg = f"æ¨¡å‹æµ‹è¯•å¤±è´¥: {str(e)}"
        print(f"âŒ {error_msg}")
        print(f"ğŸ“ Full traceback:")
        traceback.print_exc()
        
        return jsonify({
            'success': False,
            'error': error_msg
        }), 500
    finally:
        print(f"=== Model Test Finished ===\n")

import json
import os
import traceback
import time

from flask import Flask, request, Response, jsonify
from flask_cors import CORS
app = Flask(__name__)
CORS(app)

import sys
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from prompts.baseprompt import clean_txt_content, load_prompt

from core.writer_utils import KeyPointMsg
from core.draft_writer import DraftWriter
from core.plot_writer import PlotWriter
from core.outline_writer import OutlineWriter

from setting import setting_bp
from summary import process_novel
from backend_utils import get_model_config_from_provider_model
from config import MAX_NOVEL_SUMMARY_LENGTH, MAX_THREAD_NUM, ENABLE_ONLINE_DEMO

# å¯¼å…¥åŠ¨æ€é…ç½®API
try:
    from dynamic_config_api import dynamic_config_bp
    app.register_blueprint(dynamic_config_bp)
    print("âœ… åŠ¨æ€é…ç½®APIå·²æ³¨å†Œ")
except ImportError as e:
    print(f"âš ï¸ åŠ¨æ€é…ç½®APIå¯¼å…¥å¤±è´¥: {e}")

app.register_blueprint(setting_bp)

# æ·»åŠ é…ç½®
BACKEND_HOST = os.environ.get('BACKEND_HOST', '0.0.0.0')
BACKEND_PORT = int(os.environ.get('BACKEND_PORT', 7869))


@app.route('/health', methods=['GET'])
def health_check():
    return jsonify({
        'status': 'healthy',
        'timestamp': int(time.time())
    }), 200


def load_novel_writer(writer_mode, chunk_list, global_context, x_chunk_length, y_chunk_length, main_model, sub_model, max_thread_num) -> DraftWriter:
    import traceback
    
    print(f"\n=== Loading Novel Writer ===")
    print(f"Writer Mode: {writer_mode}")
    print(f"Main Model: {main_model}")
    print(f"Sub Model: {sub_model}")
    
    try:
        print(f"ğŸ”§ Getting main model config...")
        main_model_config = get_model_config_from_provider_model(main_model)
        print(f"âœ… Main model config obtained")
        
        print(f"ğŸ”§ Getting sub model config...")
        sub_model_config = get_model_config_from_provider_model(sub_model)
        print(f"âœ… Sub model config obtained")
        
        kwargs = dict(
            xy_pairs=chunk_list,
            model=main_model_config,
            sub_model=sub_model_config,
        )

        kwargs['x_chunk_length'] = x_chunk_length
        kwargs['y_chunk_length'] = y_chunk_length
        kwargs['max_thread_num'] = max_thread_num
        
        print(f"ğŸ”§ Creating writer for mode: {writer_mode}")
        
        match writer_mode:
            case 'draft':
                kwargs['global_context'] = {}
                novel_writer = DraftWriter(**kwargs)
            case 'outline':
                kwargs['global_context'] = {'summary': global_context}
                novel_writer = OutlineWriter(**kwargs)
            case 'plot':
                kwargs['global_context'] = {'chapter': global_context}
                novel_writer = PlotWriter(**kwargs)
            case _:
                error_msg = f"unknown writer: {writer_mode}"
                print(f"âŒ {error_msg}")
                raise ValueError(error_msg)
                
        print(f"âœ… Novel writer created successfully")
        return novel_writer
        
    except Exception as e:
        print(f"âŒ Error loading novel writer: {type(e).__name__}: {str(e)}")
        traceback.print_exc()
        raise
    finally:
        print(f"=== Novel Writer Loading Finished ===\n")





prompt_names = dict(
    outline = ['æ–°å»ºç« èŠ‚', 'æ‰©å†™ç« èŠ‚', 'æ¶¦è‰²ç« èŠ‚'],
    plot = ['æ–°å»ºå‰§æƒ…', 'æ‰©å†™å‰§æƒ…', 'æ¶¦è‰²å‰§æƒ…'],
    draft = ['æ–°å»ºæ­£æ–‡', 'æ‰©å†™æ­£æ–‡', 'æ¶¦è‰²æ­£æ–‡'],
)

# è·å–é¡¹ç›®æ ¹ç›®å½•
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))

prompt_dirname = dict(
    outline = os.path.join(project_root, 'prompts', 'åˆ›ä½œç« èŠ‚'),
    plot = os.path.join(project_root, 'prompts', 'åˆ›ä½œå‰§æƒ…'),
    draft = os.path.join(project_root, 'prompts', 'åˆ›ä½œæ­£æ–‡'),
)

# Enhanced prompt directories
enhanced_prompt_dirname = dict(
    outline = os.path.join(project_root, 'prompts', 'enhanced', 'åˆ›ä½œç« èŠ‚'),
    plot = os.path.join(project_root, 'prompts', 'enhanced', 'åˆ›ä½œå‰§æƒ…'),
    draft = os.path.join(project_root, 'prompts', 'enhanced', 'åˆ›ä½œæ­£æ–‡'),
)


PROMPTS = {}
# Load regular prompts
for type_name, dirname in prompt_dirname.items():
    PROMPTS[type_name] = {'prompt_names': prompt_names[type_name]}
    for name in prompt_names[type_name]:
        try:
            content = clean_txt_content(load_prompt(dirname, name))
            if content.startswith("user:\n"):
                content = content[len("user:\n"):]
            PROMPTS[type_name][name] = {'content': content}
        except Exception as e:
            print(f"âš ï¸ åŠ è½½æç¤ºè¯æ–‡ä»¶å¤±è´¥: {dirname}/{name}.txt - {e}")
            # ä½¿ç”¨é»˜è®¤å†…å®¹ç»§ç»­è¿è¡Œè€Œä¸æ˜¯å´©æºƒ
            PROMPTS[type_name][name] = {'content': f"# {name}\n\næç¤ºè¯æ–‡ä»¶åŠ è½½å¤±è´¥: {e}"}

# Load enhanced prompts
PROMPTS['enhanced'] = {}
for type_name, dirname in enhanced_prompt_dirname.items():
    PROMPTS['enhanced'][type_name] = {'prompt_names': prompt_names[type_name]}
    for name in prompt_names[type_name]:
        try:
            content = clean_txt_content(load_prompt(dirname, name))
            if content.startswith("user:\n"):
                content = content[len("user:\n"):]
            PROMPTS['enhanced'][type_name][name] = {'content': content}
        except Exception as e:
            print(f"âš ï¸ åŠ è½½å¢å¼ºæç¤ºè¯æ–‡ä»¶å¤±è´¥: {dirname}/{name}.txt - {e}")
            # Fallback to regular prompt if enhanced version fails
            if type_name in PROMPTS and name in PROMPTS[type_name]:
                PROMPTS['enhanced'][type_name][name] = PROMPTS[type_name][name]
            else:
                PROMPTS['enhanced'][type_name][name] = {'content': f"# {name}\n\nå¢å¼ºæç¤ºè¯æ–‡ä»¶åŠ è½½å¤±è´¥: {e}"}


@app.route('/prompts', methods=['GET'])
def get_prompts():
    return jsonify(PROMPTS)

@app.route('/generate_enhanced_prompts', methods=['POST'])
def generate_enhanced_prompts():
    """Generate enhanced prompts - since they're already created, just return success"""
    try:
        # Check if enhanced prompts directory exists and has content
        enhanced_exists = all(
            os.path.exists(dirname) and 
            any(os.path.exists(os.path.join(dirname, f"{name}.txt")) for name in prompt_names[type_name])
            for type_name, dirname in enhanced_prompt_dirname.items()
        )
        
        if enhanced_exists:
            return jsonify({'success': True, 'message': 'å¢å¼ºæç¤ºè¯å·²å¯ç”¨'})
        else:
            return jsonify({'success': False, 'error': 'å¢å¼ºæç¤ºè¯æ–‡ä»¶æœªæ‰¾åˆ°'})
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)})

def get_delta_chunks(prev_chunks, curr_chunks):
    """Calculate delta between previous and current chunks"""
    if not prev_chunks or len(prev_chunks) != len(curr_chunks):
        return "init", curr_chunks
    
    # Check if all strings in current chunks start with their corresponding previous strings
    is_delta = True
    for prev_chunk, curr_chunk in zip(prev_chunks, curr_chunks):
        if len(prev_chunk) != len(curr_chunk):
            is_delta = False
            break
        for prev_str, curr_str in zip(prev_chunk, curr_chunk):
            if not curr_str.startswith(prev_str):
                is_delta = False
                break
        if not is_delta:
            break
    
    if not is_delta:
        return "init", curr_chunks
    
    # Calculate deltas
    delta_chunks = []
    for prev_chunk, curr_chunk in zip(prev_chunks, curr_chunks):
        delta_chunk = []
        for prev_str, curr_str in zip(prev_chunk, curr_chunk):
            delta_str = curr_str[len(prev_str):]
            delta_chunk.append(delta_str)
        delta_chunks.append(delta_chunk)
    
    return "delta", delta_chunks


def call_write(writer_mode, chunk_list, global_context, chunk_span, prompt_content, x_chunk_length, y_chunk_length, main_model, sub_model, max_thread_num, only_prompt):
    import traceback
    
    print(f"\n{'='*60}")
    print(f"ğŸš€ Novel Writing Process Started")
    print(f"{'='*60}")
    print(f"ğŸ“ Writer Mode: {writer_mode}")
    print(f"ğŸ¤– Main Model: {main_model}")
    print(f"ğŸ¤– Sub Model: {sub_model}")
    print(f"ğŸ”¢ Max Thread Num: {max_thread_num}")
    print(f"ğŸ“‹ Chunk List Length: {len(chunk_list) if chunk_list else 0}")
    print(f"ğŸ“„ Global Context Length: {len(str(global_context)) if global_context else 0} characters")
    print(f"ğŸ“„ Global Context Preview: {str(global_context)[:200]}...")
    print(f"âš™ï¸ Chunk Span: {chunk_span}")
    print(f"ğŸ“ X Chunk Length: {x_chunk_length}")
    print(f"ğŸ“ Y Chunk Length: {y_chunk_length}")
    print(f"ğŸ¯ Only Prompt: {only_prompt}")
    print(f"ğŸ’¼ Prompt Content Length: {len(prompt_content) if prompt_content else 0} characters")
    
    # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨LM Studioæœ¬åœ°æ¨¡å‹
    if main_model and ('localhost' in main_model or '127.0.0.1' in main_model):
        print(f"ğŸ  æ£€æµ‹åˆ°LM Studioæœ¬åœ°æ¨¡å‹ï¼Œå°†ä½¿ç”¨å»¶é•¿çš„è¶…æ—¶æ—¶é—´")
        
    start_time = time.time()
    print(f"â° å¼€å§‹æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(start_time))}")
    print(f"{'='*60}")
    print(f"âœ… åˆå§‹åŒ–å®Œæˆ")
    print(f"{'='*60}\n")
    
    # ç»Ÿè®¡ä¿¡æ¯
    total_processed_chunks = 0
    total_api_calls = 0
    total_cost = 0.0
    total_chars_generated = 0
    
    try:
        if ENABLE_ONLINE_DEMO:
            if max_thread_num > MAX_THREAD_NUM:
                error_msg = "åœ¨çº¿Demoæ¨¡å‹ä¸‹ï¼Œæœ€å¤§çº¿ç¨‹æ•°ä¸èƒ½è¶…è¿‡" + str(MAX_THREAD_NUM) + "ï¼"
                print(f"âŒ å‚æ•°éªŒè¯å¤±è´¥: {error_msg}")
                raise Exception(error_msg)
        
        print(f"ğŸ”„ æ­£åœ¨å¤„ç†chunkåˆ—è¡¨...")
        # è¾“å…¥çš„chunk_listä¸­æ¯ä¸ªchunkéœ€è¦åŠ ä¸Šæ¢è¡Œï¼Œé™¤äº†æœ€åä¸€ä¸ªchunkï¼ˆå› ä¸ºæ˜¯ä»é¡µé¢ä¸­å„ä¸ªchunkä¼ æ¥çš„ï¼‰
        original_chunk_count = len(chunk_list)
        chunk_list = [[e.strip() + ('\n' if e.strip() and rowi != len(chunk_list)-1 else '') for e in row] for rowi, row in enumerate(chunk_list)]
        print(f"âœ… Chunkåˆ—è¡¨é¢„å¤„ç†å®Œæˆï¼Œå…±å¤„ç† {original_chunk_count} ä¸ªchunk")

        prev_chunks = None
        def delta_wrapper(chunk_list, done=False, msg=None):
            # è¿”å›çš„chunk_listä¸­æ¯ä¸ªchunkéœ€è¦å»æ‰æ¢è¡Œ
            chunk_list = [[e.strip() for e in row] for row in chunk_list]

            nonlocal prev_chunks
            if prev_chunks is None:
                prev_chunks = chunk_list
                return {
                    "done": done,
                    "chunk_type": "init",
                    "chunk_list": chunk_list,
                    "msg": msg
                }
            else:
                chunk_type, new_chunks = get_delta_chunks(prev_chunks, chunk_list)
                prev_chunks = chunk_list
                return {
                    "done": done,
                    "chunk_type": chunk_type,
                    "chunk_list": new_chunks,
                    "msg": msg
                }
            
        print(f"ğŸ”§ æ­£åœ¨åŠ è½½å°è¯´å†™ä½œå™¨...")
        writer_load_start = time.time()
        novel_writer = load_novel_writer(writer_mode, chunk_list, global_context, x_chunk_length, y_chunk_length, main_model, sub_model, max_thread_num)
        writer_load_time = time.time() - writer_load_start
        print(f"âœ… å°è¯´å†™ä½œå™¨åŠ è½½å®Œæˆï¼Œè€—æ—¶: {writer_load_time:.2f}ç§’")
        
    except Exception as e:
        print(f"âŒ call_write åˆå§‹åŒ–å¤±è´¥:")
        print(f"   é”™è¯¯ç±»å‹: {type(e).__name__}")
        print(f"   é”™è¯¯ä¿¡æ¯: {str(e)}")
        print(f"   å®Œæ•´å †æ ˆè·Ÿè¸ª:")
        traceback.print_exc()
        raise
    
    # draftéœ€è¦æ˜ å°„ï¼Œæ‰€ä»¥è¿›è¡Œåˆå§‹åˆ’åˆ†
    if writer_mode == 'draft':
        print(f"ğŸ“ å¤„ç†draftæ¨¡å¼çš„ç‰¹æ®Šé€»è¾‘...")
        target_chunk = novel_writer.get_chunk(pair_span=chunk_span)
        new_target_chunk = novel_writer.map_text_wo_llm(target_chunk)
        novel_writer.apply_chunks([target_chunk], [new_target_chunk])
        chunk_span = novel_writer.get_chunk_pair_span(new_target_chunk)
        print(f"âœ… Draftæ¨¡å¼åˆå§‹åŒ–å®Œæˆï¼Œæ›´æ–°åçš„chunk_span: {chunk_span}")
    
    init_novel_writer = load_novel_writer(writer_mode, list(novel_writer.xy_pairs), global_context, x_chunk_length, y_chunk_length, main_model, sub_model, max_thread_num)
    
    # TODO: writer.write åº”è¯¥ä¿è¯æ— è®ºä»€ä¹ˆpromptï¼Œéƒ½èƒ½å¤ŸåŒæ—¶é€‚åº”yä¸ºç©ºå’Œyæœ‰å€¼åœ°æƒ…å†µ
    # æ¢å¥è¯è¯´ï¼Œå°±æ˜¯è™½ç„¶å¯ä»¥å•åˆ—å‡ºä¸€ä¸ª"æ–°å»ºæ­£æ–‡"ï¼Œä½†ç”¨æ‰©å†™æ­£æ–‡ä¹Ÿèƒ½å®ç°åŒæ ·çš„æ•ˆæœã€‚
    print(f"ğŸ¯ å¼€å§‹æ‰§è¡Œå†™ä½œç”Ÿæˆå™¨...")
    generator = novel_writer.write(prompt_content, pair_span=chunk_span) 
    
    prompt_outputs = []
    last_yield_time = time.time()  # Initialize the last yield time
    last_progress_info = None  # Track last progress to avoid duplicates
    step_count = 0

    prompt_name = ''
    for kp_msg in generator:
        if isinstance(kp_msg, KeyPointMsg):
            # å¦‚æœè¦æ”¯æŒå…³é”®èŠ‚ç‚¹ä¿å­˜ï¼Œéœ€è¦è®¡ç®—ä¸€ä¸ªç¼–è¾‘ä¸Šçš„æ›´æ”¹ï¼Œç„¶ååœ¨è¿™é‡Œyield writer
            prompt_name = kp_msg.prompt_name
            step_count += 1
            print(f"ğŸ”„ æ­¥éª¤ {step_count}: å¼€å§‹æ‰§è¡Œ {prompt_name}")
            continue
        else:
            chunk_list = kp_msg

        current_cost = 0
        currency_symbol = ''
        current_model = ''
        data_chunks = []
        prompt_outputs.clear()
        
        # å¤„ç†APIè°ƒç”¨ç»“æœ
        processed_chunks = 0
        api_call_count = 0
        
        for e in chunk_list:
            if e is None: continue  # eä¸ºNoneè¯´æ˜è¯¥chunkè¿˜æœªå¤„ç†
            output, chunk = e
            if output is None: continue # outputä¸ºNoneè¯´æ˜è¯¥chunkæœªyieldå°±returnï¼Œè¯´æ˜æœªè°ƒç”¨llm
            
            processed_chunks += 1
            api_call_count += 1
            total_api_calls += 1
            
            prompt_outputs.append(output)
            current_text = ""
            current_model = output['response_msgs'].model
            chunk_cost = output['response_msgs'].cost
            current_cost += chunk_cost
            total_cost += chunk_cost
            currency_symbol = output['response_msgs'].currency_symbol
            
            # è®¡ç®—ç”Ÿæˆçš„å­—ç¬¦æ•°
            text_length = len(output.get('text', ''))
            total_chars_generated += text_length
            
            if 'plot2text' in output:
                current_text += f"æ­£åœ¨å»ºç«‹æ˜ å°„å…³ç³»..." + '\n'
            else:
                current_text = output['text']
            data_chunks.append((chunk.x_chunk, chunk.y_chunk, current_text))
            
        total_processed_chunks += processed_chunks
        
        if only_prompt:
            print(f"âœ… ä»…æŸ¥çœ‹Promptæ¨¡å¼ï¼Œè¿”å› {len(prompt_outputs)} ä¸ªPrompt")
            yield {'prompts': [e['response_msgs'] for e in prompt_outputs]}
            return

        current_time = time.time()
        step_elapsed = current_time - last_yield_time
        
        # Create progress info tuple to check for duplicates
        progress_info = (prompt_name, len(prompt_outputs), len(chunk_list), current_model, current_cost)
        
        if current_time - last_yield_time >= 0.2 and progress_info != last_progress_info:  # Check time and avoid duplicates
            progress_msg = f"æ­£åœ¨ {prompt_name} ï¼ˆ{len(prompt_outputs)} / {len(chunk_list)}ï¼‰"
            if current_model:
                progress_msg += f" æ¨¡å‹ï¼š{current_model} èŠ±è´¹ï¼š{current_cost:.5f}{currency_symbol}"
            
            # æ›´è¯¦ç»†çš„æ§åˆ¶å°æ—¥å¿—
            print(f"ğŸ“Š {'='*50}")
            print(f"ğŸ“Š åˆ›ä½œè¿›åº¦æ›´æ–° - æ­¥éª¤ {step_count}")
            print(f"ğŸ“Š {'='*50}")
            print(f"ğŸ“ å½“å‰æ­¥éª¤: {prompt_name}")
            print(f"ğŸ”¢ å¤„ç†è¿›åº¦: {len(prompt_outputs)} / {len(chunk_list)} ä¸ªå—")
            print(f"ğŸ¤– ä½¿ç”¨æ¨¡å‹: {current_model}")
            print(f"ğŸ’° å½“å‰èŠ±è´¹: {current_cost:.5f}{currency_symbol}")
            print(f"â±ï¸ æ­¥éª¤ç”¨æ—¶: {step_elapsed:.2f}ç§’")
            print(f"ğŸ§± æ•°æ®å—æ•°é‡: {len(data_chunks)}")
            print(f"ğŸ“Š APIè°ƒç”¨æ¬¡æ•°: {api_call_count}")
            print(f"ğŸ“„ ç”Ÿæˆå­—ç¬¦æ•°: {sum(len(chunk[2]) for chunk in data_chunks)}")
            print(f"{'='*50}")
            
            # ç´¯è®¡ç»Ÿè®¡ä¿¡æ¯
            print(f"ğŸ“ˆ ç´¯è®¡ç»Ÿè®¡ä¿¡æ¯:")
            print(f"   ğŸ“Š æ€»å¤„ç†å—æ•°: {total_processed_chunks}")
            print(f"   ğŸ”„ æ€»APIè°ƒç”¨æ•°: {total_api_calls}")
            print(f"   ğŸ’° æ€»èŠ±è´¹: {total_cost:.5f}{currency_symbol}")
            print(f"   ğŸ“„ æ€»ç”Ÿæˆå­—ç¬¦æ•°: {total_chars_generated}")
            print(f"   â±ï¸ æ€»ç”¨æ—¶: {current_time - start_time:.2f}ç§’")
            print(f"{'='*50}\n")
            
            yield delta_wrapper(data_chunks, done=False, msg=progress_msg)
            last_yield_time = current_time  # Update the last yield time
            last_progress_info = progress_info  # Update the last progress info

    # æœ€ç»ˆå¤„ç†
    print(f"ğŸ”„ æ­£åœ¨è®¡ç®—æœ€ç»ˆå·®å¼‚...")
    diff_start = time.time()
    # è¿™é‡Œæ˜¯è®¡ç®—å‡ºä¸€ä¸ªç¼–è¾‘ä¸Šçš„æ›´æ”¹ï¼Œæ–¹ä¾¿å‰ç«¯æ˜¾ç¤ºï¼Œåç»­diffåŠŸèƒ½å°†ä¸ç”±writeræä¾›ï¼Œå› ä¸ºè¿™æ˜¯ä¸ºäº†æ˜¾ç¤ºçš„è¦æ±‚
    data_chunks = init_novel_writer.diff_to(novel_writer, pair_span=chunk_span)
    diff_time = time.time() - diff_start
    print(f"âœ… å·®å¼‚è®¡ç®—å®Œæˆï¼Œè€—æ—¶: {diff_time:.2f}ç§’")
    
    # æœ€ç»ˆç»Ÿè®¡ä¿¡æ¯
    total_time = time.time() - start_time
    print(f"\n{'='*60}")
    print(f"ğŸ‰ å°è¯´åˆ›ä½œè¿‡ç¨‹å®Œæˆï¼")
    print(f"{'='*60}")
    print(f"ğŸ“Š æœ€ç»ˆç»Ÿè®¡ä¿¡æ¯:")
    print(f"   ğŸ“ å†™ä½œæ¨¡å¼: {writer_mode}")
    print(f"   ğŸ”¢ å¤„ç†æ­¥éª¤æ•°: {step_count}")
    print(f"   ğŸ“Š æ€»å¤„ç†å—æ•°: {total_processed_chunks}")
    print(f"   ğŸ”„ æ€»APIè°ƒç”¨æ•°: {total_api_calls}")
    print(f"   ğŸ’° æ€»èŠ±è´¹: {total_cost:.5f}{currency_symbol}")
    print(f"   ğŸ“„ æ€»ç”Ÿæˆå­—ç¬¦æ•°: {total_chars_generated}")
    print(f"   â±ï¸ æ€»ç”¨æ—¶: {total_time:.2f}ç§’")
    print(f"   ğŸ“ˆ å¹³å‡æ¯APIè°ƒç”¨ç”¨æ—¶: {total_time/total_api_calls:.2f}ç§’" if total_api_calls > 0 else "")
    print(f"   ğŸ“ˆ å¹³å‡æ¯å­—ç¬¦æˆæœ¬: {total_cost/total_chars_generated:.8f}{currency_symbol}" if total_chars_generated > 0 else "")
    print(f"   ğŸ“ˆ ç”Ÿæˆæ•ˆç‡: {total_chars_generated/total_time:.2f} å­—ç¬¦/ç§’" if total_time > 0 else "")
    print(f"{'='*60}")
    
    yield delta_wrapper(data_chunks, done=True, msg='åˆ›ä½œå®Œæˆ!')


@app.route('/write', methods=['POST'])
def write():
    data = request.json                 
    writer_mode = data['writer_mode']
    chunk_list = data['chunk_list']
    chunk_span = data['chunk_span']
    prompt_content = data['prompt_content']
    x_chunk_length = data['x_chunk_length']
    y_chunk_length = data['y_chunk_length']
    main_model = data['main_model']
    sub_model = data['sub_model']
    global_context = data['global_context']
    only_prompt = data['only_prompt']
    
    # Update settings if provided
    if 'settings' in data:
        max_thread_num = data['settings']['MAX_THREAD_NUM']

    # Generate unique stream ID
    stream_id = str(time.time())
    active_streams[stream_id] = True
    
    # è®°å½•è¯·æ±‚ä¿¡æ¯
    print(f"\n{'='*60}")
    print(f"ğŸŒ æ”¶åˆ°/writeè¯·æ±‚")
    print(f"{'='*60}")
    print(f"ğŸ“ Writer Mode: {writer_mode}")
    print(f"ğŸ†” Stream ID: {stream_id}")
    print(f"ğŸ“‹ Chunk List Length: {len(chunk_list) if chunk_list else 0}")
    print(f"âš™ï¸ Chunk Span: {chunk_span}")
    print(f"ğŸ¤– Main Model: {main_model}")
    print(f"ğŸ¤– Sub Model: {sub_model}")
    print(f"ğŸ¯ Only Prompt: {only_prompt}")
    print(f"â° è¯·æ±‚æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())}")
    print(f"{'='*60}\n")
    
    def generate():
        request_start_time = time.time()
        try:
            # Send stream ID to client
            yield f"data: {json.dumps({'stream_id': stream_id})}\n\n"

            result_count = 0
            for result in call_write(writer_mode, list(chunk_list), global_context, chunk_span, prompt_content, x_chunk_length, y_chunk_length, main_model, sub_model, max_thread_num, only_prompt):
                if not active_streams.get(stream_id, False):
                    # Stream was stopped by client
                    print(f"â¹ï¸ Streamè¢«å®¢æˆ·ç«¯åœæ­¢: {stream_id}")
                    print(f"â±ï¸ è¿è¡Œæ—¶é—´: {time.time() - request_start_time:.2f}ç§’")
                    return
                
                result_count += 1
                if result_count <= 3:  # Log first few results
                    print(f"ğŸ“¤ å‘é€ç»“æœ #{result_count}: {str(result)[:200]}...")
                
                yield f"data: {json.dumps(result)}\n\n"
                
            print(f"âœ… /writeè¯·æ±‚å¤„ç†å®Œæˆ")
            print(f"ğŸ“Š æ€»è®¡å‘é€ {result_count} ä¸ªç»“æœ")
            print(f"â±ï¸ æ€»å¤„ç†æ—¶é—´: {time.time() - request_start_time:.2f}ç§’")
            
        except Exception as e:
            error_start_time = time.time()
            print(f"\n{'='*60}")
            print(f"âŒ /writeè¯·æ±‚å¤„ç†å¤±è´¥")
            print(f"{'='*60}")
            print(f"ğŸ†” Stream ID: {stream_id}")
            print(f"ğŸ“ Writer Mode: {writer_mode}")
            print(f"ğŸ¤– Main Model: {main_model}")
            print(f"ğŸ¤– Sub Model: {sub_model}")
            print(f"â° é”™è¯¯å‘ç”Ÿæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())}")
            print(f"â±ï¸ è¿è¡Œæ—¶é—´: {time.time() - request_start_time:.2f}ç§’")
            print(f"{'='*60}")
            print(f"âŒ é”™è¯¯ç±»å‹: {type(e).__name__}")
            print(f"âŒ é”™è¯¯ä¿¡æ¯: {str(e)}")
            print(f"âŒ å®Œæ•´å †æ ˆè·Ÿè¸ª:")
            traceback.print_exc()
            print(f"{'='*60}")
            
            # å°è¯•è·å–æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
            error_details = {
                'error_type': type(e).__name__,
                'error_message': str(e),
                'stream_id': stream_id,
                'writer_mode': writer_mode,
                'main_model': main_model,
                'sub_model': sub_model,
                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S', time.localtime()),
                'runtime': f"{time.time() - request_start_time:.2f}ç§’"
            }
            
            # å¦‚æœæœ‰ç½‘ç»œç›¸å…³é”™è¯¯ï¼Œè®°å½•æ›´å¤šä¿¡æ¯
            if hasattr(e, 'response'):
                error_details['http_status'] = getattr(e.response, 'status_code', 'Unknown')
                error_details['http_headers'] = dict(getattr(e.response, 'headers', {}))
                try:
                    error_details['http_body'] = e.response.text[:1000] if hasattr(e.response, 'text') else str(e.response)[:1000]
                except:
                    error_details['http_body'] = 'Unable to read response body'
            
            print(f"ğŸ“Š é”™è¯¯è¯¦æƒ…: {json.dumps(error_details, indent=2, ensure_ascii=False)}")
            
            error_msg = f"åˆ›ä½œå‡ºé”™ï¼š\né”™è¯¯ç±»å‹: {type(e).__name__}\né”™è¯¯ä¿¡æ¯: {str(e)}\n\nè¯¦ç»†ä¿¡æ¯:\n- Stream ID: {stream_id}\n- å†™ä½œæ¨¡å¼: {writer_mode}\n- ä¸»æ¨¡å‹: {main_model}\n- å‰¯æ¨¡å‹: {sub_model}\n- è¿è¡Œæ—¶é—´: {time.time() - request_start_time:.2f}ç§’\n- é”™è¯¯æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())}"
            
            # æ·»åŠ ç‰¹å®šé”™è¯¯ç±»å‹çš„å»ºè®®
            if 'timeout' in str(e).lower():
                error_msg += f"\n\nğŸ’¡ å»ºè®®: è¿™æ˜¯è¶…æ—¶é”™è¯¯ï¼Œå¯èƒ½æ˜¯æ¨¡å‹å“åº”è¿‡æ…¢ï¼Œå»ºè®®:\n- æ£€æŸ¥ç½‘ç»œè¿æ¥\n- å°è¯•ä½¿ç”¨æ›´å¿«çš„æ¨¡å‹\n- å‡å°‘å¤„ç†çš„æ–‡æœ¬é‡"
            elif 'api' in str(e).lower() and 'key' in str(e).lower():
                error_msg += f"\n\nğŸ’¡ å»ºè®®: è¿™æ˜¯APIå¯†é’¥ç›¸å…³é”™è¯¯ï¼Œå»ºè®®:\n- æ£€æŸ¥APIå¯†é’¥æ˜¯å¦æ­£ç¡®\n- æ£€æŸ¥APIå¯†é’¥æ˜¯å¦æœ‰æ•ˆ\n- æ£€æŸ¥è´¦æˆ·ä½™é¢"
            elif 'connection' in str(e).lower():
                error_msg += f"\n\nğŸ’¡ å»ºè®®: è¿™æ˜¯ç½‘ç»œè¿æ¥é”™è¯¯ï¼Œå»ºè®®:\n- æ£€æŸ¥ç½‘ç»œè¿æ¥\n- æ£€æŸ¥é˜²ç«å¢™è®¾ç½®\n- æ£€æŸ¥ä»£ç†è®¾ç½®"
            
            error_chunk_list = [[*e[:2], error_msg] for e in chunk_list[chunk_span[0]:chunk_span[1]]]
            
            error_data = {
                "done": True,
                "chunk_type": "init",
                "chunk_list": error_chunk_list,
                "error": True,
                "error_details": error_details
            }
            
            yield f"data: {json.dumps(error_data)}\n\n"
            
        finally:
            # Clean up stream tracking
            if stream_id in active_streams:
                del active_streams[stream_id]
                print(f"ğŸ§¹ æ¸…ç†Streamè·Ÿè¸ª: {stream_id}")

    return Response(generate(), mimetype='text/event-stream')


@app.route('/summary', methods=['POST'])
def process_novel_text():
    data = request.json
    content = data['content']
    novel_name = data['novel_name']

    # Generate unique stream ID
    stream_id = str(time.time())
    active_streams[stream_id] = True
    
    # è®°å½•è¯·æ±‚ä¿¡æ¯
    print(f"\n{'='*60}")
    print(f"ğŸŒ æ”¶åˆ°/summaryè¯·æ±‚")
    print(f"{'='*60}")
    print(f"ğŸ“š Novel Name: {novel_name}")
    print(f"ğŸ†” Stream ID: {stream_id}")
    print(f"ğŸ“„ Content Length: {len(content)} characters")
    print(f"ğŸ¤– Main Model: {data.get('main_model', 'Unknown')}")
    print(f"ğŸ¤– Sub Model: {data.get('sub_model', 'Unknown')}")
    print(f"âš™ï¸ Settings: {data.get('settings', {})}")
    print(f"â° è¯·æ±‚æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())}")
    print(f"{'='*60}\n")

    def generate():
        request_start_time = time.time()
        try:
            yield f"data: {json.dumps({'stream_id': stream_id})}\n\n"

            main_model = get_model_config_from_provider_model(data['main_model'])
            sub_model = get_model_config_from_provider_model(data['sub_model'])
            max_novel_summary_length = data['settings']['MAX_NOVEL_SUMMARY_LENGTH']
            max_thread_num = data['settings']['MAX_THREAD_NUM']
            
            print(f"ğŸ“Š å¤„ç†å‚æ•°:")
            print(f"   ğŸ“ æœ€å¤§å°è¯´é•¿åº¦: {max_novel_summary_length}")
            print(f"   ğŸ”¢ æœ€å¤§çº¿ç¨‹æ•°: {max_thread_num}")
            print(f"   ğŸ¤– ä¸»æ¨¡å‹é…ç½®: {main_model}")
            print(f"   ğŸ¤– å‰¯æ¨¡å‹é…ç½®: {sub_model}")
            
            last_yield_time = 0
            result_count = 0
            
            for result in process_novel(content, novel_name, main_model, sub_model, max_novel_summary_length, max_thread_num):
                if not active_streams.get(stream_id, False):
                    # Stream was stopped by client
                    print(f"â¹ï¸ Streamè¢«å®¢æˆ·ç«¯åœæ­¢: {stream_id}")
                    print(f"â±ï¸ è¿è¡Œæ—¶é—´: {time.time() - request_start_time:.2f}ç§’")
                    return
                
                result_count += 1
                if result_count <= 3:  # Log first few results
                    print(f"ğŸ“¤ å‘é€ç»“æœ #{result_count}: {str(result)[:200]}...")
                
                current_time = time.time()
                yield_value = f"data: {json.dumps(result)}\n\n"
                if current_time - last_yield_time >= 0.2:
                    last_yield_time = current_time
                    yield yield_value
                    
            if current_time - last_yield_time < 0.2:
                # Save last yield to yaml file
                import yaml
                result_dict = json.loads(yield_value.replace('data: ', '').strip())
                with open('tmp.yaml', 'w', encoding='utf-8') as f:
                    yaml.dump(result_dict, f, allow_unicode=True)
                    
                yield yield_value   # Ensure last yield is returned
                
            print(f"âœ… /summaryè¯·æ±‚å¤„ç†å®Œæˆ")
            print(f"ğŸ“Š æ€»è®¡å‘é€ {result_count} ä¸ªç»“æœ")
            print(f"â±ï¸ æ€»å¤„ç†æ—¶é—´: {time.time() - request_start_time:.2f}ç§’")
            
        except Exception as e:
            print(f"\n{'='*60}")
            print(f"âŒ /summaryè¯·æ±‚å¤„ç†å¤±è´¥")
            print(f"{'='*60}")
            print(f"ğŸ†” Stream ID: {stream_id}")
            print(f"ğŸ“š Novel Name: {novel_name}")
            print(f"ğŸ“„ Content Length: {len(content)} characters")
            print(f"ğŸ¤– Main Model: {data.get('main_model', 'Unknown')}")
            print(f"ğŸ¤– Sub Model: {data.get('sub_model', 'Unknown')}")
            print(f"â° é”™è¯¯å‘ç”Ÿæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())}")
            print(f"â±ï¸ è¿è¡Œæ—¶é—´: {time.time() - request_start_time:.2f}ç§’")
            print(f"{'='*60}")
            print(f"âŒ é”™è¯¯ç±»å‹: {type(e).__name__}")
            print(f"âŒ é”™è¯¯ä¿¡æ¯: {str(e)}")
            print(f"âŒ å®Œæ•´å †æ ˆè·Ÿè¸ª:")
            traceback.print_exc()
            print(f"{'='*60}")
            
            # å°è¯•è·å–æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
            error_details = {
                'error_type': type(e).__name__,
                'error_message': str(e),
                'stream_id': stream_id,
                'novel_name': novel_name,
                'content_length': len(content),
                'main_model': data.get('main_model', 'Unknown'),
                'sub_model': data.get('sub_model', 'Unknown'),
                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S', time.localtime()),
                'runtime': f"{time.time() - request_start_time:.2f}ç§’"
            }
            
            # å¦‚æœæœ‰ç½‘ç»œç›¸å…³é”™è¯¯ï¼Œè®°å½•æ›´å¤šä¿¡æ¯
            if hasattr(e, 'response'):
                error_details['http_status'] = getattr(e.response, 'status_code', 'Unknown')
                error_details['http_headers'] = dict(getattr(e.response, 'headers', {}))
                try:
                    error_details['http_body'] = e.response.text[:1000] if hasattr(e.response, 'text') else str(e.response)[:1000]
                except:
                    error_details['http_body'] = 'Unable to read response body'
            
            print(f"ğŸ“Š é”™è¯¯è¯¦æƒ…: {json.dumps(error_details, indent=2, ensure_ascii=False)}")
            
            error_msg = f"å°è¯´å¤„ç†å‡ºé”™ï¼š\né”™è¯¯ç±»å‹: {type(e).__name__}\né”™è¯¯ä¿¡æ¯: {str(e)}\n\nè¯¦ç»†ä¿¡æ¯:\n- Stream ID: {stream_id}\n- å°è¯´åç§°: {novel_name}\n- å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦\n- ä¸»æ¨¡å‹: {data.get('main_model', 'Unknown')}\n- å‰¯æ¨¡å‹: {data.get('sub_model', 'Unknown')}\n- è¿è¡Œæ—¶é—´: {time.time() - request_start_time:.2f}ç§’\n- é”™è¯¯æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())}"
            
            # æ·»åŠ ç‰¹å®šé”™è¯¯ç±»å‹çš„å»ºè®®
            if 'timeout' in str(e).lower():
                error_msg += f"\n\nğŸ’¡ å»ºè®®: è¿™æ˜¯è¶…æ—¶é”™è¯¯ï¼Œå¯èƒ½æ˜¯æ¨¡å‹å“åº”è¿‡æ…¢ï¼Œå»ºè®®:\n- æ£€æŸ¥ç½‘ç»œè¿æ¥\n- å°è¯•ä½¿ç”¨æ›´å¿«çš„æ¨¡å‹\n- å‡å°‘å¤„ç†çš„æ–‡æœ¬é‡"
            elif 'api' in str(e).lower() and 'key' in str(e).lower():
                error_msg += f"\n\nğŸ’¡ å»ºè®®: è¿™æ˜¯APIå¯†é’¥ç›¸å…³é”™è¯¯ï¼Œå»ºè®®:\n- æ£€æŸ¥APIå¯†é’¥æ˜¯å¦æ­£ç¡®\n- æ£€æŸ¥APIå¯†é’¥æ˜¯å¦æœ‰æ•ˆ\n- æ£€æŸ¥è´¦æˆ·ä½™é¢"
            elif 'connection' in str(e).lower():
                error_msg += f"\n\nğŸ’¡ å»ºè®®: è¿™æ˜¯ç½‘ç»œè¿æ¥é”™è¯¯ï¼Œå»ºè®®:\n- æ£€æŸ¥ç½‘ç»œè¿æ¥\n- æ£€æŸ¥é˜²ç«å¢™è®¾ç½®\n- æ£€æŸ¥ä»£ç†è®¾ç½®"
            elif 'é•¿åº¦' in str(e) or 'length' in str(e).lower():
                error_msg += f"\n\nğŸ’¡ å»ºè®®: è¿™æ˜¯é•¿åº¦é™åˆ¶é”™è¯¯ï¼Œå»ºè®®:\n- å‡å°‘è¾“å…¥æ–‡æœ¬çš„é•¿åº¦\n- å¢åŠ ç³»ç»Ÿçš„æœ€å¤§å¤„ç†é•¿åº¦é™åˆ¶\n- åˆ†æ®µå¤„ç†é•¿æ–‡æœ¬"
            
            error_data = {
                "progress_msg": error_msg,
                "error": True,
                "error_details": error_details
            }
            
            yield f"data: {json.dumps(error_data)}\n\n"
            
        finally:
            # Clean up stream tracking
            if stream_id in active_streams:
                del active_streams[stream_id]
                print(f"ğŸ§¹ æ¸…ç†Streamè·Ÿè¸ª: {stream_id}")

    return Response(generate(), mimetype='text/event-stream')

# Dictionary to track active streams
active_streams = {}

@app.route('/stop_stream', methods=['POST'])
def stop_stream():
    data = request.json
    stream_id = data.get('stream_id')
    if stream_id in active_streams:
        active_streams[stream_id] = False
    return jsonify({'success': True})

if __name__ == '__main__':
    app.run(host=BACKEND_HOST, port=BACKEND_PORT, debug=False) 
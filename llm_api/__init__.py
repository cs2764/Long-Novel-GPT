from typing import Dict, Any, Optional, Generator

from .mongodb_cache import llm_api_cache
from .baidu_api import stream_chat_with_wenxin, wenxin_model_config
from .doubao_api import stream_chat_with_doubao, doubao_model_config
from .chat_messages import ChatMessages
from .openai_api import stream_chat_with_gpt, gpt_model_config
from .zhipuai_api import stream_chat_with_zhipuai, zhipuai_model_config

class ModelConfig(dict):
    def __init__(self, model: str, **options):
        super().__init__(**options)
        self['model'] = model
        # Add system_prompt if not provided
        if 'system_prompt' not in self:
            self['system_prompt'] = ""
        # Add timeout configuration based on provider
        self._set_timeout_by_provider()
        self.validate()
    
    def _set_timeout_by_provider(self):
        """æ ¹æ®æä¾›å•†ç±»å‹è®¾ç½®è¶…æ—¶æ—¶é—´"""
        if 'timeout' not in self:
            # ç»Ÿä¸€è®¾ç½®æ‰€æœ‰æä¾›å•†çš„è¶…æ—¶æ—¶é—´ä¸º300ç§’
            self['timeout'] = 300  # æ‰€æœ‰æä¾›å•†ï¼š5åˆ†é’Ÿ
            print(f"ğŸ• è®¾ç½®APIæ¨¡å‹è¶…æ—¶æ—¶é—´ä¸º5åˆ†é’Ÿ")

    def validate(self):
        def check_key(provider, keys):
            for key in keys:    
                if key not in self:
                    raise ValueError(f"{provider}çš„APIè®¾ç½®ä¸­æœªä¼ å…¥: {key}")
                elif not self[key].strip():
                    raise ValueError(f"{provider}çš„APIè®¾ç½®ä¸­æœªé…ç½®: {key}")

        if self['model'] in wenxin_model_config:
            check_key('æ–‡å¿ƒä¸€è¨€', ['ak', 'sk'])
        elif self['model'] in doubao_model_config:
            check_key('è±†åŒ…', ['api_key', 'endpoint_id'])
        elif self['model'] in zhipuai_model_config:
            check_key('æ™ºè°±AI', ['api_key'])
        elif self['model'] in gpt_model_config or True:
            # å…¶ä»–æ¨¡å‹åé»˜è®¤é‡‡ç”¨openaiæ¥å£è°ƒç”¨
            check_key('OpenAI', ['api_key'])
        
        if 'max_tokens' not in self:
            raise ValueError('ModelConfigæœªä¼ å…¥key: max_tokens')
        else:
            assert self['max_tokens'] <= 8_192, 'max_tokensæœ€å¤§ä¸º8192ï¼'


    def get_api_keys(self) -> Dict[str, str]:
        return {k: v for k, v in self.items() if k not in ['model']}

@llm_api_cache()
def stream_chat(model_config: ModelConfig, messages: list, response_json=False) -> Generator:
    import json
    
    print(f"\n=== LLM API Stream Chat Started ===")
    print(f"Model Config: {json.dumps(dict(model_config), default=str, ensure_ascii=False)}")
    print(f"Messages Count: {len(messages)}")
    print(f"Response JSON: {response_json}")
    
    try:
        if isinstance(model_config, dict):
            model_config = ModelConfig(**model_config)
        
        model_config.validate()
        print(f"âœ… Model config validated successfully")

        # Inject system prompt if provided
        if model_config.get('system_prompt') and model_config['system_prompt'].strip():
            # Add system prompt as the first message if not already present
            if not messages or messages[0].get('role') != 'system':
                system_message = {'role': 'system', 'content': model_config['system_prompt']}
                messages = [system_message] + list(messages)
                print(f"âœ… System prompt injected: {model_config['system_prompt'][:100]}...")
            else:
                # If first message is already a system message, prepend our system prompt
                existing_system = messages[0]['content']
                combined_system = f"{model_config['system_prompt']}\n\n{existing_system}"
                messages[0]['content'] = combined_system
                print(f"âœ… System prompt prepended to existing system message")

        messages = ChatMessages(messages, model=model_config['model'])
        print(f"âœ… Chat messages processed, token count: {messages.count_message_tokens()}")

        assert model_config['max_tokens'] <= 8192, 'max_tokensæœ€å¤§ä¸º8192ï¼'

        if messages.count_message_tokens() > model_config['max_tokens']:
            error_msg = f'è¯·æ±‚çš„æ–‡æœ¬è¿‡é•¿ï¼Œè¶…è¿‡æœ€å¤§tokens:{model_config["max_tokens"]}ã€‚'
            print(f"âŒ Token limit exceeded: {error_msg}")
            raise Exception(error_msg)
        
        yield messages
        
        model_name = model_config['model']
        print(f"ğŸ¯ Routing to appropriate API provider for model: {model_name}")
        
        if model_name in wenxin_model_config:
            print(f"ğŸ“¡ Using Wenxin API for model: {model_name}")
            result = yield from stream_chat_with_wenxin(
                messages,
                model=model_config['model'],
                ak=model_config['ak'],
                sk=model_config['sk'],
                max_tokens=model_config['max_tokens'],
                response_json=response_json
            )
        elif model_name in doubao_model_config:  # doubao models
            print(f"ğŸ“¡ Using Doubao API for model: {model_name}")
            result = yield from stream_chat_with_doubao(
                messages,
                model=model_config['model'],
                endpoint_id=model_config['endpoint_id'],
                api_key=model_config['api_key'],
                max_tokens=model_config['max_tokens'],
                response_json=response_json
            )
        elif model_name in zhipuai_model_config:  # zhipuai models
            print(f"ğŸ“¡ Using ZhipuAI API for model: {model_name}")
            result = yield from stream_chat_with_zhipuai(
                messages,
                model=model_config['model'],
                api_key=model_config['api_key'],
                max_tokens=model_config['max_tokens'],
                response_json=response_json
            )
        elif model_name in gpt_model_config or True:  # openai modelsæˆ–å…¶ä»–å…¼å®¹openaiæ¥å£çš„æ¨¡å‹
            print(f"ğŸ“¡ Using OpenAI-compatible API for model: {model_name}")
            print(f"Base URL: {model_config.get('base_url', 'Default OpenAI')}")
            result = yield from stream_chat_with_gpt(
                messages,
                model=model_config['model'],
                api_key=model_config['api_key'],
                base_url=model_config.get('base_url'),
                proxies=model_config.get('proxies'),
                max_tokens=model_config['max_tokens'],
                timeout=model_config.get('timeout', 300),
                response_json=response_json
            )
        else:
            error_msg = f"Unsupported model: {model_name}"
            print(f"âŒ {error_msg}")
            raise Exception(error_msg)
        
        result.finished = True
        yield result
        
        print(f"âœ… Stream chat completed successfully")
        return result
        
    except Exception as e:
        print(f"âŒ Stream chat failed: {type(e).__name__}: {str(e)}")
        import traceback
        traceback.print_exc()
        raise
    finally:
        print(f"=== LLM API Stream Chat Finished ===\n")

def test_stream_chat(model_config: ModelConfig):
    import json
    
    print(f"\n=== Testing Model Configuration ===")
    print(f"Test Model Config: {json.dumps(dict(model_config), default=str, ensure_ascii=False)}")
    
    try:
        messages = [{"role": "user", "content": "1+1=?ç›´æ¥è¾“å‡ºç­”æ¡ˆå³å¯ï¼š"}]
        print(f"Test message: {messages[0]}")
        
        response_count = 0
        for response in stream_chat(model_config, messages):
            response_count += 1
            if response_count <= 3:  # Log first few responses for debugging
                if hasattr(response, 'response'):
                    print(f"ğŸ“¦ Test response #{response_count}: {str(response.response)[:100]}...")
                else:
                    print(f"ğŸ“¦ Test response #{response_count}: {str(response)[:100]}...")
            
            # Return the response content
            if hasattr(response, 'response'):
                yield response.response
            elif isinstance(response, list) and len(response) > 0:
                # Handle chat messages format
                yield response[-1].get('content', '')
        
        print(f"âœ… Model test completed successfully, received {response_count} responses")
        return response
        
    except Exception as e:
        print(f"âŒ Model test failed: {type(e).__name__}: {str(e)}")
        import traceback
        traceback.print_exc()
        raise
    finally:
        print(f"=== Model Test Finished ===\n")

# å¯¼å‡ºå¿…è¦çš„å‡½æ•°å’Œé…ç½®
__all__ = ['ChatMessages', 'stream_chat', 'wenxin_model_config', 'doubao_model_config', 'gpt_model_config', 'zhipuai_model_config', 'ModelConfig']
